
<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>mygrad.tensor_base &#8212; MyGrad 2.3.0.post1.dev16 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/mygrad/tensor_base';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">MyGrad 2.3.0.post1.dev16 documentation</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../install.html">
                        Installing MyGrad
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../intro.html">
                        Introducing MyGrad
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor.html">
                        MyGradâ€™s Tensor
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../views.html">
                        Views and In-Place Operations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../performance_tips.html">
                        Performance Tips
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../operation.html">
                        Writing Your Own Operations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor_creation.html">
                        Tensor creation routines (mygrad.tensor_creation)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor_manipulation.html">
                        Tensor manipulation routines (mygrad.tensor_manip)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../linalg.html">
                        Linear algebra (mygrad.linalg)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../math.html">
                        Mathematical functions (mygrad.math)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../indexing.html">
                        Indexing Routines (mygrad.indexing_routines)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../nnet.html">
                        Neural network operations (mygrad.nnet)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../io.html">
                        Input and Output
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../graph_viz.html">
                        Computational graph visualization(mygrad.computational_graph)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../changes.html">
                        Changelog
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/rsokl/MyGrad" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../install.html">
                        Installing MyGrad
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../intro.html">
                        Introducing MyGrad
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor.html">
                        MyGradâ€™s Tensor
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../views.html">
                        Views and In-Place Operations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../performance_tips.html">
                        Performance Tips
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../operation.html">
                        Writing Your Own Operations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor_creation.html">
                        Tensor creation routines (mygrad.tensor_creation)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor_manipulation.html">
                        Tensor manipulation routines (mygrad.tensor_manip)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../linalg.html">
                        Linear algebra (mygrad.linalg)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../math.html">
                        Mathematical functions (mygrad.math)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../indexing.html">
                        Indexing Routines (mygrad.indexing_routines)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../nnet.html">
                        Neural network operations (mygrad.nnet)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../io.html">
                        Input and Output
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../graph_viz.html">
                        Computational graph visualization(mygrad.computational_graph)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../changes.html">
                        Changelog
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/rsokl/MyGrad" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">mygrad.tensor_base</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for mygrad.tensor_base</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This module defines the base tensor class along with all of its essential</span>
<span class="sd">attributes and special methods. Public math methods, e.g. ``sum``, ``mean``,</span>
<span class="sd">etc., are bound to the Tensor class in ``mygrad.__init__.py``.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numbers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Integral</span><span class="p">,</span> <span class="n">Number</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Deque</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">weakref</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReferenceType</span><span class="p">,</span> <span class="n">finalize</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">mygrad._utils.duplicating_graph</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_dup</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mygrad._utils.graph_tracking</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_track</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mygrad._utils.lock_management</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_mem</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad._numpy_version</span><span class="w"> </span><span class="kn">import</span> <span class="n">NP_IS_V2</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad._tensor_core_ops.indexing</span><span class="w"> </span><span class="kn">import</span> <span class="n">GetItem</span><span class="p">,</span> <span class="n">SetItem</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">WeakRef</span><span class="p">,</span> <span class="n">WeakRefIterable</span><span class="p">,</span> <span class="n">collect_all_tensors_and_clear_grads</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.errors</span><span class="w"> </span><span class="kn">import</span> <span class="n">DisconnectedView</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.math.arithmetic.ops</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Add</span><span class="p">,</span>
    <span class="n">Divide</span><span class="p">,</span>
    <span class="n">Multiply</span><span class="p">,</span>
    <span class="n">Negative</span><span class="p">,</span>
    <span class="n">Positive</span><span class="p">,</span>
    <span class="n">Power</span><span class="p">,</span>
    <span class="n">Square</span><span class="p">,</span>
    <span class="n">Subtract</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.math.misc.ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">MatMul</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.math.sequential.ops</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">CumProd</span><span class="p">,</span>
    <span class="n">CumSum</span><span class="p">,</span>
    <span class="n">Max</span><span class="p">,</span>
    <span class="n">Mean</span><span class="p">,</span>
    <span class="n">Min</span><span class="p">,</span>
    <span class="n">Prod</span><span class="p">,</span>
    <span class="n">StdDev</span><span class="p">,</span>
    <span class="n">Sum</span><span class="p">,</span>
    <span class="n">Variance</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.operation_base</span><span class="w"> </span><span class="kn">import</span> <span class="n">Operation</span><span class="p">,</span> <span class="n">_NoValue</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.tensor_manip.array_shape.ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Ravel</span><span class="p">,</span> <span class="n">Reshape</span><span class="p">,</span> <span class="n">Squeeze</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.tensor_manip.transpose_like.ops</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">MoveAxis</span><span class="p">,</span>
    <span class="n">SwapAxes</span><span class="p">,</span>
    <span class="n">Tensor_Transpose_Property</span><span class="p">,</span>
    <span class="n">Transpose</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">DTypeLike</span><span class="p">,</span> <span class="n">DTypeLikeReals</span><span class="p">,</span> <span class="n">Index</span><span class="p">,</span> <span class="n">Shape</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;asarray&quot;</span><span class="p">,</span> <span class="s2">&quot;astensor&quot;</span><span class="p">,</span> <span class="s2">&quot;implements_numpy_override&quot;</span><span class="p">]</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">mygrad.ufuncs._ufunc_creators</span><span class="w"> </span><span class="kn">import</span> <span class="n">ufunc</span> <span class="k">as</span> <span class="n">mygrad_ufunc</span>


<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>

<span class="n">CONSTANT_ONLY_DTYPES</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_resolve_constant</span><span class="p">(</span><span class="o">*</span><span class="n">others</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Determines if `constant` should be resolved to True based on `others`.</span>
<span class="sd">    Otherwise defers to a tensor-creator to handle further resolutions based on dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">constant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">constant</span>
    <span class="k">for</span> <span class="n">other</span> <span class="ow">in</span> <span class="n">others</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">other</span><span class="o">.</span><span class="n">constant</span><span class="p">:</span>
            <span class="c1"># let subsequent tensor casting infer constant from dtype</span>
            <span class="k">return</span> <span class="kc">None</span>
    <span class="c1"># all inputs are constants</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="asarray"><a class="viewcode-back" href="../../generated/mygrad.asarray.html#mygrad.asarray">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">asarray</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">DTypeLike</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the input to an array.</span>

<span class="sd">    This docstring is adapted from that of ``numpy.asarray``</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : array_like</span>
<span class="sd">        Input data, in any form - including a mygrad tensor - that can be converted to an array. This</span>
<span class="sd">        includes lists, lists of tuples, tuples, tuples of tuples, tuples</span>
<span class="sd">        of lists and ndarrays.</span>

<span class="sd">    dtype : data-type, optional</span>
<span class="sd">        By default, the data-type is inferred from the input data.</span>

<span class="sd">    order : {&#39;C&#39;, &#39;F&#39;}, optional</span>
<span class="sd">        Whether to use row-major (C-style) or</span>
<span class="sd">        column-major (Fortran-style) memory representation.</span>
<span class="sd">        Defaults to &#39;C&#39;.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : ndarray</span>
<span class="sd">        Array interpretation of `a`.  No copy is performed if the input</span>
<span class="sd">        is already an ndarray with matching dtype and order.  If `a` is a</span>
<span class="sd">        subclass of ndarray, a base class ndarray is returned.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Convert a list into an array:</span>

<span class="sd">    &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">    &gt;&gt;&gt; a = [1, 2]</span>
<span class="sd">    &gt;&gt;&gt; mg.asarray(a)</span>
<span class="sd">    array([1, 2])</span>

<span class="sd">    Convert a tensor into an array. No copy of the</span>
<span class="sd">    underlying numpy array is created:</span>

<span class="sd">    &gt;&gt;&gt; t = mg.Tensor([1, 2.])</span>
<span class="sd">    &gt;&gt;&gt; mg.asarray(t)</span>
<span class="sd">    array([1., 2.])</span>
<span class="sd">    &gt;&gt;&gt; t.data is np.asarray(t))</span>
<span class="sd">    True</span>

<span class="sd">    Existing arrays are not copied:</span>

<span class="sd">    &gt;&gt;&gt; a = np.array([1, 2])</span>
<span class="sd">    &gt;&gt;&gt; mg.asarray(a) is a</span>
<span class="sd">    True</span>

<span class="sd">    If `dtype` is set, array is copied only if dtype does not match:</span>

<span class="sd">    &gt;&gt;&gt; a = np.array([1, 2], dtype=np.float32)</span>
<span class="sd">    &gt;&gt;&gt; mg.asarray(a, dtype=np.float32) is a</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; mg.asarray(a, dtype=np.float64) is a</span>
<span class="sd">    False</span>

<span class="sd">    Contrary to `asanyarray`, ndarray subclasses are not passed through:</span>

<span class="sd">    &gt;&gt;&gt; issubclass(np.recarray, np.ndarray)</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([(1.0, 2), (3.0, 4)], dtype=&#39;f4,i4&#39;).view(np.recarray)</span>
<span class="sd">    &gt;&gt;&gt; mg.asarray(a) is a</span>
<span class="sd">    False</span>
<span class="sd">    &gt;&gt;&gt; np.asanyarray(a) is a</span>
<span class="sd">    True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># faster than passing the tensor directly</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor"><a class="viewcode-back" href="../../generated/mygrad.tensor.html#mygrad.tensor">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">tensor</span><span class="p">(</span>
    <span class="n">arr_like</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">DTypeLikeReals</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">ndmin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a tensor</span>

<span class="sd">    This documentation was adapted from that of ``numpy.array`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    arr_like : array_like</span>
<span class="sd">        A tensor, any object exposing the array interface, an object whose</span>
<span class="sd">        __array__ method returns an tensor, a real number, any (nested) sequence.</span>

<span class="sd">    dtype : data-type, optional</span>
<span class="sd">        The desired data-type for the tensor. Restricted to integer and float type.</span>
<span class="sd">        If not specified, then the type will be determined as the minimum type required</span>
<span class="sd">        to hold the objects in the sequence.</span>

<span class="sd">    constant : Optional[bool]</span>
<span class="sd">        If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">        facilitate back propagation (i.e. ``constant_tensor.grad`` will always</span>
<span class="sd">        return ``None``).</span>

<span class="sd">        If a new tensor is returned:</span>
<span class="sd">         - Defaults to ``False`` for float-type data.</span>
<span class="sd">         - Defaults to ``True`` for integer-type data.</span>

<span class="sd">    copy : bool, optional</span>
<span class="sd">        If true (default), or if a copy is needed to satisfy any of the</span>
<span class="sd">        other requirements (``dtype``, ``constant``, etc.) then a new tensor</span>
<span class="sd">        is created from copied data. Otherwise the tensor will be returned</span>
<span class="sd">        unchanged.</span>

<span class="sd">    ndmin : int, optional</span>
<span class="sd">        Specifies the minimum number of dimensions that the resulting</span>
<span class="sd">        tensor should have. Ones will be prepended to the shape as</span>
<span class="sd">        needed to meet this requirement.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : Tensor</span>
<span class="sd">        A tensor satisfying the specified requirements.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    empty_like : Return an empty tensor with shape and type of input.</span>
<span class="sd">    ones_like : Return an tensor of ones with shape and type of input.</span>
<span class="sd">    zeros_like : Return an tensor of zeros with shape and type of input.</span>
<span class="sd">    full_like : Return a new tensor with shape of input filled with value.</span>
<span class="sd">    empty : Return a new uninitialized tensor.</span>
<span class="sd">    ones : Return a new tensor setting values to one.</span>
<span class="sd">    zeros : Return a new tensor setting values to zero.</span>
<span class="sd">    full : Return a new tensor of given shape filled with value.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">    &gt;&gt;&gt; mg.tensor([1, 2, 3])</span>
<span class="sd">    Tensor([1, 2, 3])</span>

<span class="sd">    Upcasting:</span>

<span class="sd">    &gt;&gt;&gt; mg.tensor([1, 2, 3.0])</span>
<span class="sd">    Tensor([ 1.,  2.,  3.])</span>

<span class="sd">    More than one dimension:</span>

<span class="sd">    &gt;&gt;&gt; mg.tensor([[1, 2], [3, 4]])</span>
<span class="sd">    Tensor([[1, 2],</span>
<span class="sd">            [3, 4]])</span>

<span class="sd">    Minimum dimensions 2:</span>

<span class="sd">    &gt;&gt;&gt; mg.tensor([1, 2, 3], ndmin=2)</span>
<span class="sd">    Tensor([[1, 2, 3]])</span>

<span class="sd">    Type provided:</span>

<span class="sd">    &gt;&gt;&gt; mg.tensor([1, 2, 3], dtype=&quot;float32&quot;)</span>
<span class="sd">    Tensor([1., 2., 3.], dtype=float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arr_like</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">copy</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">arr_like</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="n">constant</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">arr_like</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ndmin</span><span class="p">,</span> <span class="n">Integral</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;TypeError: `ndmin` requires a non-negative integer (got type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ndmin</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">ndmin</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ndmin</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># numpy does this</span>
            <span class="k">if</span> <span class="n">ndmin</span> <span class="o">&gt;</span> <span class="n">arr_like</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                <span class="n">arr_like</span> <span class="o">=</span> <span class="n">arr_like</span><span class="p">[(</span><span class="o">*</span><span class="p">(</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndmin</span> <span class="o">-</span> <span class="n">arr_like</span><span class="o">.</span><span class="n">ndim</span><span class="p">)),)]</span>
            <span class="c1"># return tensor as-as</span>
            <span class="k">return</span> <span class="n">arr_like</span>

    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">arr_like</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="n">ndmin</span><span class="p">)</span></div>


<div class="viewcode-block" id="astensor"><a class="viewcode-back" href="../../generated/mygrad.astensor.html#mygrad.astensor">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">astensor</span><span class="p">(</span>
    <span class="n">t</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">DTypeLikeReals</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the input to a tensor.</span>

<span class="sd">    A tensor `t` is returned unchanged - its gradient and computational</span>
<span class="sd">    graph state preserved - if dtype and constant are compatible.</span>
<span class="sd">    A copy of the underlying numpy array is created only if dtype is</span>
<span class="sd">    incompatible or if a non-constant tensor is being created from a constant.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    t : array_like</span>
<span class="sd">        Input data, in any form that can be converted to a tensor. This</span>
<span class="sd">        includes lists, lists of tuples, tuples, tuples of tuples, tuples</span>
<span class="sd">        of lists and ndarrays.</span>

<span class="sd">    dtype : data-type, optional</span>
<span class="sd">        By default, the data-type is inferred from the input data.</span>

<span class="sd">    constant : Optional[bool]</span>
<span class="sd">        By default, `constant` is inferred from `t` if `t` is a tensor,</span>
<span class="sd">        otherwise it defaults to `False`.</span>

<span class="sd">        Defaults to ``False`` for float-type data.</span>
<span class="sd">        Defaults to ``True`` for integer-type data.</span>

<span class="sd">        Integer-type tensors must be constant.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : Tensor</span>
<span class="sd">        Tensor interpretation of `a`.  No copy is performed if the input</span>
<span class="sd">        is already a tensor with matching dtype and constant-flag.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Convert a list into an array:</span>

<span class="sd">    &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; t = [1, 2]</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(t)</span>
<span class="sd">    Tensor([1, 2])</span>

<span class="sd">    Convert an array into a tensor. No copy of the</span>
<span class="sd">    underlying numpy array is created:</span>

<span class="sd">    &gt;&gt;&gt; a = np.array([1.0, 2.0])</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(a)</span>
<span class="sd">    Tensor([1., 2.])</span>
<span class="sd">    &gt;&gt;&gt; a is mg.astensor(a).data</span>
<span class="sd">    True</span>

<span class="sd">    Existing tensors are not copied and their gradients and</span>
<span class="sd">    computational graphs are preserved:</span>

<span class="sd">    &gt;&gt;&gt; t1 = 2 * mg.tensor([1, 2])</span>
<span class="sd">    &gt;&gt;&gt; t2 = mg.astensor(t1)</span>
<span class="sd">    &gt;&gt;&gt; t1 is t2</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; t1.creator is t2.creator</span>
<span class="sd">    True</span>

<span class="sd">    If `dtype` is set, a new tensor is created - with copied data - only</span>
<span class="sd">    if dtype does not match:</span>

<span class="sd">    &gt;&gt;&gt; t = mg.Tensor([1, 2], dtype=np.float32)</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(t, dtype=np.float32) is t</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(t, dtype=np.float64) is t</span>
<span class="sd">    False</span>

<span class="sd">    Otherwise, if `constant` is set, a new tensor is created (with</span>
<span class="sd">    no copy of the underlying data) only if constant doesn&#39;t match.</span>

<span class="sd">    &gt;&gt;&gt; t1 = mg.tensor([1, 2], constant=False)</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(t1, constant=False) is t</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(t1, constant=True) is t1</span>
<span class="sd">    False</span>
<span class="sd">    &gt;&gt;&gt; mg.astensor(t1, constant=True).data is t1.data</span>
<span class="sd">    True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>


<span class="n">_REGISTERED_UFUNC</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ufunc</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;mygrad_ufunc&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_REGISTERED_DIFFERENTIABLE_NUMPY_FUNCS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span>
    <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">]</span>
<span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">_REGISTERED_BOOL_ONLY_UFUNC</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ufunc</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">isnat</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">signbit</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">greater</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">less</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">less_equal</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">not_equal</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># These are ufuncs that users might mistake for being differentiable functions;</span>
<span class="c1"># for this reason we make explicit the fact that only constant tensors are permitted</span>
<span class="c1"># in these operations.</span>
<span class="n">_REGISTERED_CONST_ONLY_UFUNC</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">np</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">remainder</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">mod</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">fmod</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">divmod</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">rint</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">trunc</span><span class="p">,</span>
<span class="p">}</span>


<span class="n">_REGISTERED_NO_DIFF_NUMPY_FUNCS</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">can_cast</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">copyto</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">may_share_memory</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">min_scalar_type</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">result_type</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">shares_memory</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">class</span><span class="w"> </span><span class="nc">implements_numpy_override</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Registers a mygrad-based override for a NumPy function of the same name, via</span>
<span class="sd">    the standard __array_function__ interface. [1]_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; @implements_numpy_override()  # np.reshape to be overridden</span>
<span class="sd">    ... def reshape(x, shape):</span>
<span class="sd">    ...    # a mygrad-based implementation of numpy.reshape</span>
<span class="sd">    ...    print(&quot;hello world&quot;)</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">    &gt;&gt;&gt; np.reshape(mg.tensor(1.), 2)</span>
<span class="sd">    &#39;hello world&#39;</span>

<span class="sd">    You can also explicit provide the numpy function explicitly</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; @implements_numpy_override(np.reshape)  # np.reshape to be overridden</span>
<span class="sd">    ... def some_function(x, shape):</span>
<span class="sd">    ...    pass</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] https://numpy.org/devdocs/reference/arrays.classes.html?#numpy.class.__array_function__</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;numpy_func&quot;</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numpy_func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1"># if None, `numpy_func` is inferred from the name of the decorated function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">numpy_func</span> <span class="o">=</span> <span class="n">numpy_func</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">wrapped_func</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy_func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">numpy_func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">np</span><span class="p">,</span> <span class="n">wrapped_func</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;@implements_numpy_override tried to register an override for the function numpy.</span><span class="si">{</span><span class="n">wrapped_func</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">, but no &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;such function exists.&quot;</span>
                <span class="p">)</span>

        <span class="n">_REGISTERED_DIFFERENTIABLE_NUMPY_FUNCS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy_func</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapped_func</span>
        <span class="k">return</span> <span class="n">wrapped_func</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_ConstantOnly</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_as_constant_array</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Passes through all non-tensor objects and constant tensors. Raises on</span>
<span class="sd">    non-constant tensors.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">_ConstantOnly</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">data</span>
    <span class="k">return</span> <span class="n">t</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A numpy-array-like object capable of serving as a node in a computational</span>
<span class="sd">    graph that supports back-propagation of derivatives via the chain rule.</span>
<span class="sd">    See the Examples section of the docstring for more details.</span>

<span class="sd">    Like the numpy array, mygrad&#39;s tensor stores data as an N-dimensional array</span>
<span class="sd">    and provides an interface accessing, setting, and performing vectorized</span>
<span class="sd">    operations along the various dimensions of this array. Vectorized operations</span>
<span class="sd">    support numpy-style broadcasting semantics.</span>

<span class="sd">    The contents of a tensor can be accessed and written to using all variety</span>
<span class="sd">    of basic and advanced indexing (along with mixtures of the two).</span>

<span class="sd">    Creating a Tensor</span>
<span class="sd">    -----------------</span>
<span class="sd">    ``mygrad.Tensor`` can be passed any &quot;array-like&quot; object of numerical data.</span>
<span class="sd">    This includes numbers, sequences (e.g. lists), nested sequences, numpy-ndarrays,</span>
<span class="sd">    and other mygrad-tensors. mygrad also provides familiar numpy-style tensor-creation</span>
<span class="sd">    functions (e.g. ``mygrad.arange``, ``mygrad.linspace``, etc.)</span>

<span class="sd">    &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">    &gt;&gt;&gt; mg.tensor(2.3)  # creating a 0-dimensional tensor</span>
<span class="sd">    Tensor(2.3)</span>
<span class="sd">    &gt;&gt;&gt; mg.tensor(np.array([1.2, 3.0]))  # casting a numpy-array to a tensor</span>
<span class="sd">    Tensor([1.2, 3.0])</span>
<span class="sd">    &gt;&gt;&gt; mg.tensor([[1, 2], [3, 4]])  # creating a 2-dimensional tensor</span>
<span class="sd">    Tensor([[1, 2],</span>
<span class="sd">            [3, 4]])</span>
<span class="sd">    &gt;&gt;&gt; mg.arange(4)    # using numpy-style tensor creation functions</span>
<span class="sd">    Tensor([0, 1, 2, 3])</span>

<span class="sd">    Creating a non-constant tensor will copy array data:</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; arr = np.arange(10.)</span>
<span class="sd">    &gt;&gt;&gt; t_var = tensor(arr, constant=False)</span>
<span class="sd">    &gt;&gt;&gt; np.shares_memory(arr, t_var)</span>
<span class="sd">    False</span>

<span class="sd">    Creating constant tensor will not make a copy of the array data:</span>

<span class="sd">    &gt;&gt;&gt; t_const = mg.tensor(arr, constant=True)</span>
<span class="sd">    &gt;&gt;&gt; np.shares_memory(arr, t_const)</span>
<span class="sd">    True</span>

<span class="sd">    Forward and Back-Propagation</span>
<span class="sd">    ----------------------------</span>
<span class="sd">    Let&#39;s construct a computational graph consisting of two zero-dimensional</span>
<span class="sd">    tensors, ``x`` and ``y``, which are used to compute an output tensor,</span>
<span class="sd">    ````. This is a &quot;forward pass imperative&quot; style for creating a computational</span>
<span class="sd">    graph - the graph is constructed as we carry out the forward-pass computation.</span>

<span class="sd">    &gt;&gt;&gt; x = mg.tensor(3.0)</span>
<span class="sd">    &gt;&gt;&gt; y = mg.tensor(2.0)</span>
<span class="sd">    &gt;&gt;&gt; â„’ = 2 * x + y ** 2</span>

<span class="sd">    Invoking ``â„’.backward()`` signals the computational graph to</span>
<span class="sd">    compute the total-derivative of ``f`` with respect to each one of its dependent</span>
<span class="sd">    variables. I.e. ``x.grad`` will store ``dâ„’/dx`` and ``y.grad`` will store</span>
<span class="sd">    ``dâ„’/dy``. Thus we have back-propagated a gradient from ``f`` through our graph.</span>

<span class="sd">    Each tensor of derivatives is computed elementwise. That is, if `x = Tensor(x0, x1, x2)`,</span>
<span class="sd">    then dâ„’/dx represents `[dâ„’/d(x0), dâ„’/d(x1), dâ„’/d(x2)]`</span>

<span class="sd">    &gt;&gt;&gt; â„’.backward()  # computes df/dx and df/dy</span>
<span class="sd">    &gt;&gt;&gt; x.grad  # df/dx</span>
<span class="sd">    array(6.0)</span>
<span class="sd">    &gt;&gt;&gt; y.grad  # df/dy</span>
<span class="sd">    array(4.0)</span>
<span class="sd">    &gt;&gt;&gt; â„’.grad</span>
<span class="sd">    array(1.0)  # dâ„’/dâ„’</span>

<span class="sd">    Once the gradients are computed, the computational graph containing ``x``,</span>
<span class="sd">    ``y``, and ``â„’`` is cleared automatically. Additionally, involving any</span>
<span class="sd">    of these tensors in a new computational graph will automatically null</span>
<span class="sd">    their gradients.</span>

<span class="sd">    &gt;&gt;&gt; 2 * x</span>
<span class="sd">    &gt;&gt;&gt; x.grad is None</span>
<span class="sd">    True</span>

<span class="sd">    Or, you can use the ``tensor.null_grad()`` method to manually clear a</span>
<span class="sd">    tensor&#39;s gradient</span>

<span class="sd">    &gt;&gt;&gt; y.null_grad()</span>
<span class="sd">    Tensor(2.)</span>
<span class="sd">    &gt;&gt;&gt; y.grad is None</span>
<span class="sd">    True</span>

<span class="sd">    Accessing the Underlying NumPy Array</span>
<span class="sd">    ------------------------------------</span>
<span class="sd">    ``mygrad.Tensor`` is a thin wrapper on ``numpy.ndarray``. A tensor&#39;s</span>
<span class="sd">    underlying numpy-array can be accessed via ``.data``:</span>

<span class="sd">    &gt;&gt;&gt; x = mg.tensor([1, 2])</span>
<span class="sd">    &gt;&gt;&gt; x.data</span>
<span class="sd">    array([1, 2])</span>

<span class="sd">    **Do not modify this underlying array**. Any in-place modifications made to this</span>
<span class="sd">    array will not be tracked by any computational graph involving that tensor, thus</span>
<span class="sd">    back-propagation through that tensor will likely be incorrect.</span>

<span class="sd">    Producing a &quot;View&quot; of a Tensor</span>
<span class="sd">    ------------------------------</span>
<span class="sd">    MyGrad&#39;s tensors exhibit the same view semantics and memory-sharing relationships</span>
<span class="sd">    as NumPy arrays. I.e. any (non-scalar) tensor produced via basic indexing will share</span>
<span class="sd">    memory with its parent.</span>

<span class="sd">    &gt;&gt;&gt; x = mg.tensor([1., 2., 3., 4.])</span>
<span class="sd">    &gt;&gt;&gt; y = x[:2]  # the view: Tensor([1., 2.])</span>
<span class="sd">    &gt;&gt;&gt; y.base is x</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; np.shares_memory(x, y)</span>
<span class="sd">    True</span>

<span class="sd">    Mutating shared data will propagate through views:</span>

<span class="sd">    &gt;&gt;&gt; y *= -1</span>
<span class="sd">    &gt;&gt;&gt; x</span>
<span class="sd">    Tensor([-1., -2.,  3.,  4.])</span>
<span class="sd">    &gt;&gt;&gt; y</span>
<span class="sd">    Tensor([-1., -2.])</span>

<span class="sd">    And this view relationship will also manifest between the tensors&#39; gradients</span>

<span class="sd">    &gt;&gt;&gt; (x ** 2).backward()</span>
<span class="sd">    &gt;&gt;&gt; x.grad</span>
<span class="sd">    array([-2., -4.,  6.,  8.])</span>
<span class="sd">    &gt;&gt;&gt; y.grad</span>
<span class="sd">    array([-2., -4.])</span>

<span class="sd">    In-Place Operations are not Efficient</span>
<span class="sd">    =====================================</span>
<span class="sd">    It is important to note that while MyGrad&#39;s view semantics promote a rich parity</span>
<span class="sd">    with NumPy, that certain aspects should be avoided in the interest of optimized performance.</span>
<span class="sd">    Namely, performing in-place operations on tensors is generally not more efficient than</span>
<span class="sd">    their non-mutating counterparts.</span>

<span class="sd">    This is because MyGrad has to track the state of tensors that are involved in a computational</span>
<span class="sd">    graph. Thus a mutated tensor must have its pre-augmented state stored for future reference; this</span>
<span class="sd">    defeats the performance benefit of writing to an array&#39;s memory in-place. This is especially</span>
<span class="sd">    inefficient if you are mutating a tensor involved with multiple views of the same memory(</span>
<span class="sd">    By contrast, producing a view of a tensor _is_ efficient as one would expect).</span>

<span class="sd">    Thus these NumPy-like in-place semantics are supported by MyGrad not for the same performance</span>
<span class="sd">    purposes, but instead to support convenient and familiar code-patterns and to enable one to</span>
<span class="sd">    port NumPy code to MyGrad (or, in the future, inject MyGrad tensors into NumPy!!) and get</span>
<span class="sd">    the exact same behavior.</span>

<span class="sd">    A final note: MyGrad&#39;s in-place operations, when run under :func:`~mygrad.no_autodiff` mode,</span>
<span class="sd">    do not incur the extra costs noted above, and thus your code will benefit from the performance</span>
<span class="sd">    benefits of in-place operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mf">15.0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__array_ufunc__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">ufunc</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ufunc</span><span class="p">],</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;An interface provided by NumPy to override the behavior of its ufuncs [1]_.</span>

<span class="sd">        MyGrad implements its own ufuncs for all differentiable NumPy ufuncs.</span>

<span class="sd">        Non-differentiable numpy ufuncs simply get called on the underlying arrays of tensors and</span>
<span class="sd">        will return ndarrays.</span>

<span class="sd">        The differentiability - or lack thereof - of ufuncs may not be obvious to end users.</span>
<span class="sd">        Thus potentially ambiguous ufuncs (e.g. `numpy.ceil`) will be made to raise on non-constant</span>
<span class="sd">        tensors so that the lack of differentiability is made obvious to the users. This design decision</span>
<span class="sd">        is made in the same spirit as requiring integer-dtype tensors be constant.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        .. [1] https://numpy.org/doc/stable/reference/arrays.classes.html#numpy.class.__array_ufunc__</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        NumPy ufuncs that represent differentiable operations are overloaded by MyGrad tensors</span>
<span class="sd">        so that they support backprop</span>

<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>

<span class="sd">        &gt;&gt;&gt; x = mg.tensor([1., 2.])</span>

<span class="sd">        This calls ``mygrad.sin`` under the hood.</span>

<span class="sd">        &gt;&gt;&gt; np.sin(x)  # returns a tensor</span>
<span class="sd">        Tensor([0.84147098, 0.90929743])</span>

<span class="sd">        &gt;&gt;&gt; np.sin(x).backward()</span>
<span class="sd">        &gt;&gt;&gt; x.grad  # stores d(sin(x))/dx @ x = [1., 2.]</span>
<span class="sd">        array([ 0.54030231, -0.41614684])</span>

<span class="sd">        Specifying a dtype, a ``where`` mask, an in-place target (via ``out``) as an array</span>
<span class="sd">        or a tensor, are all supported.</span>

<span class="sd">        &gt;&gt;&gt; x = mg.tensor([1., 2.])</span>
<span class="sd">        &gt;&gt;&gt; y = mg.tensor([-1., -1.])</span>
<span class="sd">        &gt;&gt;&gt; np.exp(x, where=[False, True], out=y)</span>
<span class="sd">        Tensor([-1.       ,  7.3890561])</span>
<span class="sd">        &gt;&gt;&gt; y.backward()</span>
<span class="sd">        &gt;&gt;&gt; x.grad</span>
<span class="sd">        array([0.       , 7.3890561])</span>

<span class="sd">        Non-differentiable NumPy ufuncs simply operate on the ndarrays that are wrapped</span>
<span class="sd">        by MyGrad tensors; these return ndarrays, which will appropriately and explicitly</span>
<span class="sd">        serve as constants elsewhere in a computational graph.</span>

<span class="sd">        &gt;&gt;&gt; x = mg.tensor([1., 2.])</span>
<span class="sd">        &gt;&gt;&gt; np.less_equal(x, 1)</span>
<span class="sd">        array([ True, False])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;mygrad does not support in-place operations with more that one target&quot;</span>
            <span class="p">)</span>
        <span class="p">(</span><span class="n">out</span><span class="p">,)</span> <span class="o">=</span> <span class="n">out</span>

        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">]]</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># differentiable ufunc implemented by mygrad</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_REGISTERED_UFUNC</span><span class="p">[</span><span class="n">ufunc</span><span class="p">],</span> <span class="n">method</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># non-differentiable ufuncs get called on numpy arrays stored by tensors</span>
        <span class="k">if</span> <span class="n">ufunc</span> <span class="ow">in</span> <span class="n">_REGISTERED_BOOL_ONLY_UFUNC</span><span class="p">:</span>
            <span class="n">caster</span> <span class="o">=</span> <span class="n">asarray</span>
        <span class="k">elif</span> <span class="n">ufunc</span> <span class="ow">in</span> <span class="n">_REGISTERED_CONST_ONLY_UFUNC</span><span class="p">:</span>
            <span class="c1"># the presence of non-constant tensors will raise</span>
            <span class="n">caster</span> <span class="o">=</span> <span class="n">_as_constant_array</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">caster</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="c1"># returns ndarray</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ufunc</span><span class="p">,</span> <span class="n">method</span><span class="p">)(</span><span class="o">*</span><span class="p">(</span><span class="n">caster</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">_ConstantOnly</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">ufunc</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot involve non-constant mygrad tensors.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__array_function__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">_REGISTERED_DIFFERENTIABLE_NUMPY_FUNCS</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_REGISTERED_DIFFERENTIABLE_NUMPY_FUNCS</span><span class="p">[</span><span class="n">func</span><span class="p">](</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">_REGISTERED_NO_DIFF_NUMPY_FUNCS</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span>
                <span class="o">*</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">args</span><span class="p">),</span>
                <span class="o">**</span><span class="p">{</span>
                    <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">},</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__array__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">DTypeLike</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">copy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">NP_IS_V2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
            <span class="k">if</span> <span class="n">copy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">copy</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">DTypeLikeReals</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">ndmin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Operation</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : ArrayLike</span>
<span class="sd">            Input data, in any form that can be converted to an array.  This</span>
<span class="sd">            includes numbers, sequences, nested sequences, numpy-ndarrays,</span>
<span class="sd">            and mygrad-tensors.</span>

<span class="sd">        dtype : DTypeLikeReals</span>
<span class="sd">            `int`, `float`, or a real-valued numpy data type. By default the</span>
<span class="sd">            data type is inferred from ``x`` via ``numpy.asarray(x)``.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. `self.grad` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        copy : Optional[bool]</span>
<span class="sd">            Determines if the incoming array-data will be copied.</span>

<span class="sd">        ndmin : int, optional</span>
<span class="sd">            Specifies the minimum number of dimensions that the resulting</span>
<span class="sd">            array should have.  Ones will be prepended to the shape as</span>
<span class="sd">            needed to meet this requirement.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The following are parameters reserved only for internal use:</span>

<span class="sd">        _creator : Optional[mygrad.Operation]</span>
<span class="sd">            The operation-instance whose forward pass produced `self`. Should not</span>
<span class="sd">            be set manually by users.</span>

<span class="sd">        _base : Optional[Tensor]</span>
<span class="sd">            Points to the tensor that ``self`` shares memory with.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">constant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constant</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`constant` must be a boolean value, got: </span><span class="si">{</span><span class="n">constant</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Operation</span><span class="p">]</span> <span class="o">=</span> <span class="n">_creator</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">NP_IS_V2</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="n">ndmin</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">copy</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ndmin</span><span class="p">,</span> <span class="n">Integral</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ndmin</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39; object cannot be interpreted as an integer&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">ndmin</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="n">ndmin</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span>
                        <span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndmin</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ndim</span><span class="p">)),)</span>
                    <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span> <span class="n">ndmin</span><span class="o">=</span><span class="n">ndmin</span><span class="p">)</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span>
        <span class="n">is_float</span> <span class="o">=</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floating</span><span class="p">)</span>  <span class="c1"># faster than `numpy.issubdtype`</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_float</span> <span class="ow">and</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span><span class="p">:</span>
            <span class="c1"># No need to constrain dtypes if we aren&#39;t tracking the graph.</span>
            <span class="c1"># Also, it is nice to enable complex arithmetic through mygrad</span>
            <span class="c1"># functions that are wrapped in no_autodiff</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">CONSTANT_ONLY_DTYPES</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Tensor data must be of an floating type, integer type, or boolean type, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;received </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="k">elif</span> <span class="n">constant</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Integer-valued tensors must be treated as constants.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># non-float: default constant -&gt; True</span>
            <span class="c1"># float: default constant -&gt; False</span>
            <span class="n">constant</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">is_float</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_constant</span> <span class="o">=</span> <span class="n">constant</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: Union[None, np.ndarray]</span>

        <span class="c1"># track all operations that this tensor participates in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">WeakRef</span><span class="p">[</span><span class="n">Operation</span><span class="p">]]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="c1"># base points to the initial tensor that owns the memory of this</span>
        <span class="c1"># tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="o">=</span> <span class="n">_base</span>  <span class="c1"># type: Optional[Tensor]</span>
        <span class="c1"># stores all of the tensors that are a view of this tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_view_children</span> <span class="o">=</span> <span class="n">WeakRefIterable</span><span class="p">()</span>  <span class="c1"># type: WeakRefIterable[Tensor]</span>

        <span class="c1"># Used to reflect the view of the gradient associated with that of `self.base`.</span>
        <span class="c1"># This is a means of distinguishing between the gradient set on `self` as</span>
        <span class="c1"># part of backpropagation and the view of the gradient of its base.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the derivative of ``â„’`` with respect to this tensor.</span>

<span class="sd">        ``â„’`` is the terminal node in the compuational graph from which</span>
<span class="sd">        ``â„’.backward()`` was invoked.</span>

<span class="sd">        If this tensor is a view of another tensor then their gradients</span>
<span class="sd">        will exhibit the same memory-sharing relationship as their data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dâ„’/dx: numpy.ndarray</span>
<span class="sd">            The gradient of the terminal node in a computational graph</span>
<span class="sd">            with respect to this tensor. The shape of this numpy array</span>
<span class="sd">            matches ``self.shape``</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([1.0, 2.0])</span>

<span class="sd">        Prior to backpropagation tensors have ``None`` set for their gradients.</span>

<span class="sd">        &gt;&gt;&gt; x.grad is None</span>
<span class="sd">        True</span>

<span class="sd">        Now we trigger backpropagation...</span>

<span class="sd">        &gt;&gt;&gt; â„’ = x ** 2</span>
<span class="sd">        &gt;&gt;&gt; â„’.backward()</span>

<span class="sd">        and we see that ``x.grad`` stores dâ„’/dx</span>

<span class="sd">        &gt;&gt;&gt; x.grad  # dâ„’/dx</span>
<span class="sd">        array([2., 4.])</span>

<span class="sd">        Now we will demonstrate the relationship between gradient a view tensor</span>
<span class="sd">        and that of its base.</span>

<span class="sd">        &gt;&gt;&gt; base = mg.Tensor([1.0, 2.0, 3.0])</span>
<span class="sd">        &gt;&gt;&gt; view = base[:2]; view</span>
<span class="sd">        Tensor([1., 2.])</span>

<span class="sd">        &gt;&gt;&gt; â„’ = base ** 2</span>
<span class="sd">        &gt;&gt;&gt; â„’.backward()</span>

<span class="sd">        Although ``view`` is not directly involved in the computation in ``â„’``,</span>
<span class="sd">        and thus would not typically store a gradient in due to ``â„’.backward()``,</span>
<span class="sd">        it shares memory with ``base`` and thus it stores a gradient in correspondence</span>
<span class="sd">        to this &quot;view relationship&quot;. I.e. because ``view == base[:2]``, then we expect</span>
<span class="sd">        to find that ``view.grad == base.grad[:2]``.</span>

<span class="sd">        &gt;&gt;&gt; base.grad</span>
<span class="sd">        array([2., 4., 6.])</span>
<span class="sd">        &gt;&gt;&gt; view.grad</span>
<span class="sd">        array([2., 4.])</span>

<span class="sd">        &gt;&gt;&gt; view.grad.base is base.grad</span>
<span class="sd">        True</span>

<span class="sd">        The reasoning here is that, because a base tensor and its view share the same</span>
<span class="sd">        array data, then varying an element in that data implies that both the base</span>
<span class="sd">        tensor and the view will change (assuming the variation occurs specifically in</span>
<span class="sd">        a shared region). It follows that the base tensor&#39;s gradient must share the same</span>
<span class="sd">        relationship with the view-tensor since these are measures of &quot;cause and effects&quot;</span>
<span class="sd">        associated with varying elements of data (albeit infinitesmaly).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">_grad</span><span class="p">:</span>
            <span class="c1"># view grad has been computed already</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1">#  ``self`` had its graph, connecting it to its base, cleared.</span>
            <span class="c1">#  ``self._view_grad`` can&#39;t be computed without this info.</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="p">(</span><span class="n">view_parent</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span><span class="o">.</span><span class="n">variables</span>

        <span class="c1"># recursively fetches grad from parent</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">view_parent</span><span class="o">.</span><span class="n">grad</span>
        <span class="k">with</span> <span class="n">_track</span><span class="o">.</span><span class="n">no_autodiff</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_op</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span>

<div class="viewcode-block" id="Tensor.astype"><a class="viewcode-back" href="../../generated/mygrad.Tensor.astype.html#mygrad.Tensor.astype">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">astype</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">DTypeLikeReals</span><span class="p">,</span>
        <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;unsafe&quot;</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Copy of the tensor with the specified dtype.</span>

<span class="sd">        The resulting tensor is not involved in any computational graph</span>
<span class="sd">        and has no gradient associated with it.</span>

<span class="sd">        This docstring was adapted from that of ``ndarray.astype``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        dtype : Union[type, str]</span>
<span class="sd">            The real-valued numeric data type. This can be a numpy dtype or</span>
<span class="sd">            a corresponding string identifier.</span>

<span class="sd">        casting : Literal[&#39;no&#39;, &#39;equiv&#39;, &#39;safe&#39;, &#39;same_kind&#39;, &#39;unsafe&#39;]</span>
<span class="sd">            Controls what kind of data casting may occur. Defaults to â€˜unsafeâ€™ for backwards compatibility.</span>
<span class="sd">                - â€˜noâ€™ means the data types should not be cast at all.</span>
<span class="sd">                - â€˜equivâ€™ means only byte-order changes are allowed.</span>
<span class="sd">                - â€˜safeâ€™ means only casts which can preserve values are allowed.</span>
<span class="sd">                - â€˜same_kindâ€™ means only safe casts or casts within a kind, like float64 to float32, are allowed.</span>
<span class="sd">                - â€˜unsafeâ€™ means any data conversions may be done.</span>

<span class="sd">        copy : bool, optional (default=True)</span>
<span class="sd">            By default, astype always returns a newly allocated array. If this is set to false, and</span>
<span class="sd">            the ``dtype`` and ``constant`` requirements are satisfied, the input tensor is returned</span>
<span class="sd">            instead of a copy.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If specified, determines if the returned tensor is a constant.</span>
<span class="sd">            Otherwise this argument is inferred from the original tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            The resulting tensor with the specified data type.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        [1].. Retrieved from: https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = mg.arange(10); x</span>
<span class="sd">        Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span>

<span class="sd">        Using a string to specify the data type:</span>

<span class="sd">        &gt;&gt;&gt; x.astype(&quot;float32&quot;)</span>
<span class="sd">        Tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)</span>

<span class="sd">        Specifying a numpy data type object, and specifying that the</span>
<span class="sd">        tensor is to be treated as a constant:</span>

<span class="sd">        &gt;&gt;&gt; x.astype(np.int8, constant=True)</span>
<span class="sd">        Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cast_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="n">casting</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">cast_data</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">and</span> <span class="p">(</span><span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="n">constant</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)(</span><span class="n">cast_data</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_op</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">Op</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Operation</span><span class="p">],</span>
        <span class="o">*</span><span class="n">input_vars</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">op_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">op_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wraps operations performed between tensors: f(a, b, ...).</span>

<span class="sd">        For developer use only.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Op : Type[Operation]</span>
<span class="sd">            Operation-class, used to perform forward-pass on `input_vars`.</span>

<span class="sd">        input_vars : Tuple[array_like, ...]</span>
<span class="sd">            An arbitrary number of input-tensors. These can take any form that</span>
<span class="sd">            can be converted to an array.  This includes numbers, sequences, nested</span>
<span class="sd">            numerical sequences, numpy-ndarrays, and mygrad-tensors.</span>

<span class="sd">        op_args : Optional[Tuple[Any, ...]]</span>
<span class="sd">            Arbitrary positional arguments passed to the operation&#39;s forward pass.</span>

<span class="sd">        op_kwargs : Optional[Dict[str, Any]]</span>
<span class="sd">            Arbitrary keyword arguments passed to the operation&#39;s forward pass.</span>

<span class="sd">        constant : bool, optional (default=False)</span>
<span class="sd">            If True, the resulting Tensor is a constant.</span>

<span class="sd">        out: Optional[Union[np.ndarray, &quot;Tensor&quot;]]</span>
<span class="sd">            The target where the output (an ndarray) of the operation will be written.</span>
<span class="sd">            Thus this raises if `out` is read-only.</span>

<span class="sd">            There is an exception to this if  a tensor is provided, in which case the</span>
<span class="sd">            operation does not write to its underlying memory but rather triggers</span>
<span class="sd">            &quot;in-place semantics&quot; so that the computational graph behaves as if the</span>
<span class="sd">            tensor was mutated. See  ``Tensor._in_place_op`` for more details.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>
<span class="sd">            The tensor-result of the operation&#39;s forward-pass.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;mygrad does not support in-place operations with more that one target&quot;</span>
                    <span class="p">)</span>
                <span class="p">(</span><span class="n">out</span><span class="p">,)</span> <span class="o">=</span> <span class="n">out</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">out</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span>
                    <span class="n">Op</span><span class="p">,</span>
                    <span class="o">*</span><span class="n">input_vars</span><span class="p">,</span>
                    <span class="n">op_args</span><span class="o">=</span><span class="n">op_args</span><span class="p">,</span>
                    <span class="n">op_kwargs</span><span class="o">=</span><span class="n">op_kwargs</span><span class="p">,</span>
                    <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">out</span>

        <span class="n">_uniques_bases_then_arrs</span> <span class="o">=</span> <span class="p">()</span>

        <span class="n">tensor_vars</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="bp">cls</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">var</span>
            <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">input_vars</span>
        <span class="p">)</span>

        <span class="c1"># cast all input-vars to tensors</span>
        <span class="k">if</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span> <span class="ow">and</span> <span class="n">_mem</span><span class="o">.</span><span class="n">MEM_GUARD</span><span class="p">:</span>
            <span class="c1"># lock memory of array data</span>
            <span class="n">_uniques_bases_then_arrs</span> <span class="o">=</span> <span class="n">WeakRefIterable</span><span class="p">(</span>
                <span class="n">_mem</span><span class="o">.</span><span class="n">lock_arr_writeability</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">_mem</span><span class="o">.</span><span class="n">unique_arrs_and_bases</span><span class="p">(</span><span class="n">tensor_vars</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">op_args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">op_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">op_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">op_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">f</span> <span class="o">=</span> <span class="n">Op</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">op_out</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">tensor_vars</span><span class="p">,</span> <span class="o">*</span><span class="n">op_args</span><span class="p">,</span> <span class="o">**</span><span class="n">op_kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">op_out</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">tensor_vars</span><span class="p">,</span> <span class="o">*</span><span class="n">op_args</span><span class="p">,</span> <span class="o">**</span><span class="n">op_kwargs</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span> <span class="ow">and</span> <span class="n">_mem</span><span class="o">.</span><span class="n">MEM_GUARD</span><span class="p">:</span>
                <span class="n">_mem</span><span class="o">.</span><span class="n">release_writeability_lock_on_op</span><span class="p">(</span><span class="n">_uniques_bases_then_arrs</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">e</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span><span class="p">:</span>
            <span class="c1"># execute operation without tracking creator or any graph</span>
            <span class="c1"># information</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
                <span class="n">op_out</span><span class="p">,</span>
                <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>  <span class="c1"># constant not determined by graph info</span>
                <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">_creator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">_base</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># points to parent tensor that op-output is a view of</span>
        <span class="n">base</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: Optional[Tensor]</span>

        <span class="c1"># If output of op is a view - tracks the tensor var that is</span>
        <span class="c1"># the parent of the view</span>
        <span class="n">parent_var</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Determine whether or not op was a view; if so, `base`</span>
        <span class="c1"># points to parent Tensor</span>
        <span class="n">op_out_base</span> <span class="o">=</span> <span class="n">op_out</span><span class="o">.</span><span class="n">base</span>
        <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">can_return_view</span> <span class="ow">and</span> <span class="n">op_out_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">vars_can_share_mem</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">input_vars</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">can_share_mem</span><span class="p">,</span> <span class="n">parent_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vars_can_share_mem</span><span class="p">,</span> <span class="n">tensor_vars</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">can_share_mem</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">parent_data</span> <span class="o">=</span> <span class="n">parent_var</span><span class="o">.</span><span class="n">data</span>
                <span class="n">parent_data_base</span> <span class="o">=</span> <span class="n">parent_data</span><span class="o">.</span><span class="n">base</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">op_out_base</span> <span class="ow">is</span> <span class="n">parent_data</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="p">(</span><span class="n">op_out_base</span> <span class="ow">is</span> <span class="n">parent_data_base</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="p">(</span><span class="n">op_out</span> <span class="ow">is</span> <span class="n">parent_data</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="n">parent_var</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">parent_var</span><span class="o">.</span><span class="n">_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">parent_var</span><span class="o">.</span><span class="n">_base</span> <span class="o">=</span> <span class="kc">None</span>

                    <span class="n">base</span> <span class="o">=</span> <span class="n">parent_var</span> <span class="k">if</span> <span class="n">parent_var</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">parent_var</span><span class="o">.</span><span class="n">base</span>
                    <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">parent_var</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">input_vars</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="c1"># tensor&#39;s graph has been cleared, but its base lingers</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">v</span><span class="o">.</span><span class="n">_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">v</span><span class="o">.</span><span class="n">_base</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="k">if</span> <span class="n">base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># non-view ops clear grads</span>
                    <span class="n">v</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">v</span><span class="o">.</span><span class="n">_view_grad</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># we need to be able to replay view-ops for doing in-place operations</span>
            <span class="c1"># on graphs with views</span>
            <span class="n">f</span><span class="o">.</span><span class="n">replay_args</span> <span class="o">=</span> <span class="n">op_args</span>
            <span class="n">f</span><span class="o">.</span><span class="n">replay_kwargs</span> <span class="o">=</span> <span class="n">op_kwargs</span>
            <span class="n">f</span><span class="o">.</span><span class="n">replay_force_constant</span> <span class="o">=</span> <span class="n">constant</span>

        <span class="c1"># record graph information</span>
        <span class="k">if</span> <span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="n">var</span><span class="o">.</span><span class="n">constant</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">tensor_vars</span><span class="p">):</span>
                <span class="n">constant</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">constant</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># record that a variable participated in that op</span>
        <span class="n">ref_f</span> <span class="o">=</span> <span class="n">ReferenceType</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>  <span class="c1"># type: WeakRef[Operation]</span>
        <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">tensor_vars</span><span class="p">:</span>
            <span class="n">var</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ref_f</span><span class="p">)</span>

        <span class="n">tensor_out</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">op_out</span><span class="p">,</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
            <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">_creator</span><span class="o">=</span><span class="n">f</span><span class="p">,</span>
            <span class="n">_base</span><span class="o">=</span><span class="n">base</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">parent_var</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">parent_var</span><span class="o">.</span><span class="n">_view_children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_out</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_mem</span><span class="o">.</span><span class="n">MEM_GUARD</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor_out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_mem</span><span class="o">.</span><span class="n">lock_arr_writeability</span><span class="p">(</span><span class="n">tensor_out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">base</span><span class="p">)</span>
                <span class="n">_uniques_bases_then_arrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">base</span><span class="p">)</span>
            <span class="n">_mem</span><span class="o">.</span><span class="n">lock_arr_writeability</span><span class="p">(</span><span class="n">tensor_out</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">tensor_refs</span> <span class="o">=</span> <span class="n">_uniques_bases_then_arrs</span>
            <span class="n">tensor_refs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_out</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">finalize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">_mem</span><span class="o">.</span><span class="n">release_writeability_lock_on_op</span><span class="p">,</span> <span class="n">tensor_refs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_replay_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">input_vars</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;*dev use only*</span>

<span class="sd">        Replays the op that produced `self` - called on the specified</span>
<span class="sd">        input vars&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DisconnectedView</span><span class="p">(</span>
                <span class="s2">&quot;``Tensor._replay_op(...)`` was called on a tensor without a creator.&quot;</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Please report this error at: https://github.com/rsokl/MyGrad/issues&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="p">),</span>
            <span class="o">*</span><span class="n">input_vars</span><span class="p">,</span>
            <span class="n">op_args</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="o">.</span><span class="n">replay_args</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="o">.</span><span class="n">replay_kwargs</span><span class="p">,</span>
            <span class="n">constant</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="o">.</span><span class="n">replay_force_constant</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Tensor.backward"><a class="viewcode-back" href="../../generated/mygrad.Tensor.backward.html#mygrad.Tensor.backward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArrayLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Trigger backpropagation and compute the derivatives of this tensor.</span>

<span class="sd">        Designating this tensor as the tensor â„’, compute dâ„’/dx for all (non-constant) tensors</span>
<span class="sd">        that preceded â„’ in its computational graph, and store each of these derivatives in ``x.grad``</span>
<span class="sd">        respectively.</span>

<span class="sd">        Once back-propagation is finished, the present tensor is removed from all computational</span>
<span class="sd">        graphs, and the preceding graph is cleared.</span>

<span class="sd">        If â„’ is a non-scalar tensor (i.e. ``â„’.ndim`` is greater than 0), then calling</span>
<span class="sd">        ``â„’.backward()`` will behave as if â„’ was first reduced to a scalar via summation. I.e. it</span>
<span class="sd">        will behave identically to ``â„’.sum().backward()``; this ensures that each element of any</span>
<span class="sd">        dâ„’/dx will represent a derivative of a scalar function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        grad : Optional[array_like], (must be broadcast-compatible with ``self``</span>
<span class="sd">            By default, the present tensor is treated as the terminus of the computational graph (â„’).</span>
<span class="sd">            Otherwise, one can specify a &quot;downstream&quot; derivative, representing ``dâ„’/d(self)``.</span>
<span class="sd">            This can be used to effectively connect otherwise separate computational graphs.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.tensor(2)</span>
<span class="sd">        &gt;&gt;&gt; y = mg.tensor(3)</span>
<span class="sd">        &gt;&gt;&gt; w = x * y</span>
<span class="sd">        &gt;&gt;&gt; â„’ = 2 * w</span>
<span class="sd">        &gt;&gt;&gt; â„’.backward()  # computes dâ„’/dâ„’, dâ„’/dw, dâ„’/dy, and dâ„’/dx</span>

<span class="sd">        &gt;&gt;&gt; â„’.grad  # dâ„’/df == 1 by identity</span>
<span class="sd">        array(1.)</span>
<span class="sd">        &gt;&gt;&gt; w.grad  # dâ„’/dw</span>
<span class="sd">        array(2.)</span>
<span class="sd">        &gt;&gt;&gt; y.grad # dâ„’/dy = dâ„’/dw * dw/dy</span>
<span class="sd">        array(4.)</span>
<span class="sd">        &gt;&gt;&gt; x.grad # dâ„’/dx = dâ„’/dw * dw/dx</span>
<span class="sd">        array(6.)</span>

<span class="sd">        Calling ``â„’.backward()`` from a non-scalar tensor is equivalent</span>
<span class="sd">        to first summing that tensor.</span>

<span class="sd">        &gt;&gt;&gt; tensor = mg.tensor([2.0, 4.0, 8.0])</span>
<span class="sd">        &gt;&gt;&gt; â„’ = tensor * tensor[::-1]  # [x0*x2, x1*x1, x2*x0]</span>
<span class="sd">        &gt;&gt;&gt; â„’.backward()  # behaves like â„’ = x0*x2 + x1*x1 + x2*x0</span>
<span class="sd">        &gt;&gt;&gt; tensor.grad</span>
<span class="sd">        array([16.,  8.,  4.])</span>

<span class="sd">        &gt;&gt;&gt; tensor = mg.Tensor([2.0, 4.0, 8.0])</span>
<span class="sd">        &gt;&gt;&gt; â„’ = tensor * tensor[::-1]</span>
<span class="sd">        &gt;&gt;&gt; â„’.sum().backward()</span>
<span class="sd">        &gt;&gt;&gt; tensor.grad</span>
<span class="sd">        array([16.,  8.,  4.])</span>

<span class="sd">        Specifying a value for ``grad``</span>

<span class="sd">        &gt;&gt;&gt; x = mg.Tensor(1.)</span>
<span class="sd">        &gt;&gt;&gt; x.backward(2.)</span>
<span class="sd">        &gt;&gt;&gt; x.grad  # Would normally be dâ„’/dâ„’ == 1</span>
<span class="sd">        array(2.)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">clear_graph</span><span class="p">()</span>
            <span class="k">return</span>

        <span class="n">topo_sorted_tensors</span><span class="p">:</span> <span class="n">Deque</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([])</span>
        <span class="n">seen</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="n">collect_all_tensors_and_clear_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seen</span><span class="p">,</span> <span class="n">topo_sorted_tensors</span><span class="p">)</span>

        <span class="c1"># don&#39;t set self._grad yet because there is a grad-clearing step that</span>
        <span class="c1"># occurs during graph creation</span>
        <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># `self` is guaranteed to be a tensor of floats</span>
            <span class="c1"># so we can simply cast `grad` to be the same dtype</span>
            <span class="n">_grad</span> <span class="o">=</span> <span class="n">asarray</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">_grad</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># See if grad can broadcast to `self`</span>
                    <span class="c1"># raises ValueError if not</span>
                    <span class="n">_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
                        <span class="n">_grad</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">_grad</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                        <span class="c1"># mutual broadcasting occurred</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;`tensor.backward(grad)` was passed a gradient with an incompatible shape.</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;`grad` must be broadcast-compatible with `tensor.shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">`</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Got `grad.shape=</span><span class="si">{</span><span class="n">_grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">`&quot;</span>
                    <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">_grad</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">topo_sorted_tensors</span><span class="p">:</span>
                <span class="n">t</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">clear_graph</span><span class="p">()</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        **For dev-use only**</span>

<span class="sd">        If `self` has accumulated incoming gradients from all operations in the terminal node&#39;s</span>
<span class="sd">        computational graph, back-propagate the accumulated gradient to the creator of `self`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : Set[Operation]</span>
<span class="sd">            The set of all operations relevant to the terminal node of the computational graph,</span>
<span class="sd">            which triggered back-propagation</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AssertionError</span>
<span class="sd">            Raises if the tensor and its associated gradient possess different shapes.</span>
<span class="sd">            Raises if `_backward` triggered on a tensor with gradient of `None`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;backprop, post grad-accumulation, was triggered &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;on a tensor with no gradient&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">id </span><span class="si">{</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">grad: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">creator: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ops: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">base: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;A tensor and its associated gradient must possess the same shape. Got:&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">tensor-shape: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">grad-shape: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">)</span>
        <span class="k">return</span>

<div class="viewcode-block" id="Tensor.null_grad"><a class="viewcode-back" href="../../generated/mygrad.Tensor.null_grad.html#mygrad.Tensor.null_grad">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">null_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">_clear_view_info</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets this tensor&#39;s gradient to be ``None``.</span>

<span class="sd">        This operation is performed in-place, but a reference to the</span>
<span class="sd">        tensor is returned in order to permit mapping semantics.</span>

<span class="sd">        Also removes any ``base`` reference from disconnected views.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import  mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor(2.)</span>
<span class="sd">        &gt;&gt;&gt; (x ** 2).backward()</span>
<span class="sd">        &gt;&gt;&gt; x.grad</span>
<span class="sd">        array(4.)</span>
<span class="sd">        &gt;&gt;&gt; x.null_grad()  # returns a reference of `x`</span>
<span class="sd">        Tensor(2.0)</span>
<span class="sd">        &gt;&gt;&gt; x.grad is None</span>
<span class="sd">        True&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_view_grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">_clear_view_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.null_gradients"><a class="viewcode-back" href="../../generated/mygrad.Tensor.null_gradients.html#mygrad.Tensor.null_gradients">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">null_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clear_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        **Deprecated: Tensors will automatically have their computational graphs cleared during backprop.</span>
<span class="sd">        Simply involving a tensor in a new computational graph will null its gradient.**</span>

<span class="sd">        Sets the gradient for this tensor and for all preceding tensors in the computation graph</span>
<span class="sd">        to ``None``.</span>

<span class="sd">        Additionally, the computational graph that terminates in this tensor can also be cleared</span>
<span class="sd">        during this process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        clear_graph : bool, optional (default=True)</span>
<span class="sd">            If ``True`` clear the computational graph in addition to nulling the gradients.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        It is advised to clear the computational graph when nulling gradients, i.e. invoke</span>
<span class="sd">        ``null_gradients(clear_graph=True)`` (or simply ``null_gradients()``). This de-references</span>
<span class="sd">        all intermediate operations and tensors in the computational graph and thus permits</span>
<span class="sd">        garbage collection - freeing the memory that was used by the computational graph.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.tensor(2)</span>
<span class="sd">        &gt;&gt;&gt; y = mg.tensor(3)</span>
<span class="sd">        &gt;&gt;&gt; w = x * y</span>
<span class="sd">        &gt;&gt;&gt; f = 2 * w</span>
<span class="sd">        &gt;&gt;&gt; f.backward()  # computes df/df, df/dw, df/dy, and df/dx</span>
<span class="sd">        &gt;&gt;&gt; any(tensor.grad is None for tensor in (f, w , x, y))</span>
<span class="sd">        False</span>

<span class="sd">        &gt;&gt;&gt; f.null_gradients()  # set tensor.grad to None for all tensors in the graph</span>
<span class="sd">        &gt;&gt;&gt; all(tensor.grad is None for tensor in (f, w , x, y))</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`tensor.null_gradients()` is deprecated. Calling it will raise an error &quot;</span>
            <span class="s2">&quot;in future versions of MyGrad. A tensor will automatically &quot;</span>
            <span class="s2">&quot;have its gradient nulled if you use it in a new computational graph. &quot;</span>
            <span class="s2">&quot;Or, you can call `tensor.null_grad()` to null that individual tensor&#39;s &quot;</span>
            <span class="s2">&quot;gradient.&quot;</span><span class="p">,</span>
            <span class="ne">FutureWarning</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.clear_graph"><a class="viewcode-back" href="../../generated/mygrad.Tensor.clear_graph.html#mygrad.Tensor.clear_graph">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">clear_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Removes the current tensor â€“ and tensors above it â€“ from their shared</span>
<span class="sd">        computational graph.</span>

<span class="sd">        This de-references all operations involved in the graph and the intermediate</span>
<span class="sd">        tensors that were created by it. Arrays whose memory were locked by the</span>
<span class="sd">        computational graph will have their writeability restored.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1., 2.])</span>
<span class="sd">        &gt;&gt;&gt; y = mg.multiply(2., x)</span>
<span class="sd">        &gt;&gt;&gt; x.flags.writeable, y.creator</span>
<span class="sd">        (False, &lt;mygrad.math.arithmetic.ops.Multiply at 0x224f89cac48&gt;)</span>
<span class="sd">        &gt;&gt;&gt; y.clear_graph()</span>
<span class="sd">        &gt;&gt;&gt; x.flags.writeable, y.creator</span>
<span class="sd">        (True, None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># &quot;pull&quot; on grad to force views to update their</span>
            <span class="c1"># gradients from upstream before the graph info</span>
            <span class="c1"># gets cleared</span>
            <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_view_children</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># marks tensor as &quot;visited&quot; during graph-traversal</span>

        <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">creator</span><span class="o">.</span><span class="n">variables</span><span class="p">:</span>  <span class="c1"># type: &quot;Tensor&quot;</span>
            <span class="n">var</span><span class="o">.</span><span class="n">clear_graph</span><span class="p">()</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">constant</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;If ``True``, this tensor is a constant; it will not propagate any gradient.</span>

<span class="sd">        Additionally, any tensor that is a descendant of constant tensors will also</span>
<span class="sd">        be a constant.</span>

<span class="sd">        Integer-valued tesnors, Python scalars and NumPy arrays are treated as constant</span>
<span class="sd">        tensors when included in MyGrad computational graphs.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        bool</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Constant-tensors do not back-propagate gradients:</span>

<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([1., 2.], constant=True)</span>
<span class="sd">        &gt;&gt;&gt; y = mg.Tensor([0., 3.], constant=False)</span>
<span class="sd">        &gt;&gt;&gt; f = x * y</span>
<span class="sd">        &gt;&gt;&gt; f.backward()</span>

<span class="sd">        &gt;&gt;&gt; x.grad is None  # x has no gradient</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; y.grad</span>
<span class="sd">        array([1., 2.])</span>

<span class="sd">        A tensor that is derived solely from constant tensors is also</span>
<span class="sd">        a constant:</span>

<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([1., 2.], constant=True)</span>
<span class="sd">        &gt;&gt;&gt; y = mg.Tensor([0., 3.], constant=True)</span>
<span class="sd">        &gt;&gt;&gt; z = (x + y) ** 2 - np.array([8., 7.])</span>
<span class="sd">        &gt;&gt;&gt; z.constant</span>
<span class="sd">        True</span>

<span class="sd">        Integer-valued tensors are treated as constants</span>

<span class="sd">        &gt;&gt;&gt; mg.Tensor([1, 2]).constant</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constant</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">creator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Operation</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The ``Operation`` instance that produced ``self``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        creator : Optional[Operation]</span>
<span class="sd">            The operation-instance that created the tensor, or `None`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor(3)</span>
<span class="sd">        &gt;&gt;&gt; x.creator is None</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; y = mg.Tensor(2)</span>
<span class="sd">        &gt;&gt;&gt; z = x * y  # Multiply(x, y) -&gt; z</span>
<span class="sd">        &gt;&gt;&gt; z.creator</span>
<span class="sd">         &lt;mygrad.math.arithmetic.ops.Multiply at 0x2df5a130438&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creator</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="fm">__contains__</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">Index</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">GetItem</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">item</span><span class="p">,))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">]:</span>
        <span class="c1"># In the same way that numpy doesn&#39;t let you iterate over 0-dimensional</span>
        <span class="c1"># arrays, don&#39;t allow iteration over 0-dimensional arrays.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;iteration over a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_in_place_op</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inplace_op</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Operation</span><span class="p">],</span>
        <span class="o">*</span><span class="n">input_vars</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">op_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">op_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
                <span class="n">inplace_op</span><span class="p">,</span>
                <span class="o">*</span><span class="n">input_vars</span><span class="p">,</span>
                <span class="n">op_args</span><span class="o">=</span><span class="n">op_args</span><span class="p">,</span>
                <span class="n">op_kwargs</span><span class="o">=</span><span class="n">op_kwargs</span><span class="p">,</span>
                <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
                <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1">#</span>
        <span class="c1"># **********************************************************************************</span>
        <span class="c1"># The way that in-place updates work in MyGrad is that any tensor that</span>
        <span class="c1"># is about to undergo a mutation gets &quot;cloned&quot;. Each resulting &quot;placeholder&quot;</span>
        <span class="c1"># is used to represent that tensor in any non-view operations that the tensor</span>
        <span class="c1"># was participating in. This ensures that the stateful computational graph</span>
        <span class="c1"># is not corrupted by this mutation.</span>
        <span class="c1">#</span>
        <span class="c1"># Once the placeholders have been created, they have permanently replaced the</span>
        <span class="c1"># rolls of their counterparts within the computational graph. Furthermore, they</span>
        <span class="c1"># exist only internally to the computational graph and thus cannot be the</span>
        <span class="c1"># targets of subsequent views or in-place updates.</span>
        <span class="c1">#</span>
        <span class="c1"># At this point, the &quot;original&quot; tensors merely reserve the publicly-available</span>
        <span class="c1"># Tensor-instances (husks) that the users will access. We eventually need to</span>
        <span class="c1"># populate these husks with the appropriate augmented contents and graph-history.</span>
        <span class="c1">#</span>
        <span class="c1"># Thus this method will compute the in-place operation on a new tensor, and</span>
        <span class="c1"># will create a new, internal computational graph involving the base tensor</span>
        <span class="c1"># affected by the mutation and any of its view-children. These tensors represent</span>
        <span class="c1"># the mutated tensors that the users expect to have access to.</span>
        <span class="c1">#</span>
        <span class="c1"># We must connect this new computational graph to the preceding one â€“ the one</span>
        <span class="c1"># involving the placeholders; this way we can backpropagate appropriately and</span>
        <span class="c1"># through all influencers.</span>
        <span class="c1">#</span>
        <span class="c1"># Finally we mirror each of these new tensors into the husks of the publicly</span>
        <span class="c1"># -available tensors and reroute the computational graph through them so that</span>
        <span class="c1"># the user sees that all of the relevant tensors have been augmented, and that</span>
        <span class="c1"># they are connected to the appropriate &quot;history&quot; such that backprop occurs</span>
        <span class="c1"># without error or inaccuracy.</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1"># For illustration, consider the following graph:</span>
        <span class="c1">#</span>
        <span class="c1"># ... x------[square]-- y = x**2</span>
        <span class="c1">#        \</span>
        <span class="c1">#         ---[slice]-- z = view-x</span>
        <span class="c1">#                              \</span>
        <span class="c1">#                               ---[mul]-- w = 3 * z</span>
        <span class="c1">#</span>
        <span class="c1"># Now suppose that we mutate `x` with `x[:] = 0`. This is a simpler case than</span>
        <span class="c1"># mutating a view of `x`, since `x` is already the base tensor.</span>
        <span class="c1">#  - This should not affect `y`</span>
        <span class="c1">#  - It should affect `view_x`</span>
        <span class="c1">#  - It should *not* affect `w`, which depends on `view_x` in a &quot;static&quot; way.</span>
        <span class="c1">#    I.e. the value for `w` is already resolved and is not a view of z or x.</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1"># As prescribed above, we will make the placeholders: px and pz, and we</span>
        <span class="c1"># will reroute the operations that statically depend on the old values of x and z</span>
        <span class="c1"># through these placeholders.</span>
        <span class="c1">#</span>
        <span class="c1"># Next we will have `x` point to a mutated version of itself, in accord with the</span>
        <span class="c1"># in-place update being performed, and we will subsequently recreate any</span>
        <span class="c1"># views of x (i.e. z), based off of this mutated tensor.</span>
        <span class="c1">#</span>
        <span class="c1"># The resulting graph is:</span>
        <span class="c1">#</span>
        <span class="c1">#                             ---[slice]-- z = view-x</span>
        <span class="c1">#                            /</span>
        <span class="c1">#        -----[set-item] -- x = px.copy()[:]=0</span>
        <span class="c1">#       /</span>
        <span class="c1"># ... px------[square]-- y = px**2</span>
        <span class="c1">#        \</span>
        <span class="c1">#         ---[slice]-- pz = view-px</span>
        <span class="c1">#                              \</span>
        <span class="c1">#                               ---[mul]-- w = 3 * pz</span>
        <span class="c1">#</span>
        <span class="c1"># Note that px and pz are strictly *internal* tensors; they cannot be accessed for</span>
        <span class="c1"># use in any further operations, whereas `x` and `z` are available for further use.</span>
        <span class="c1">#</span>
        <span class="c1"># **********************************************************************************</span>
        <span class="c1">#</span>
        <span class="c1"># Replace base and all of its views with &quot;placeholder&quot; tensors;</span>
        <span class="c1"># they serve as internal references to all tensors pre-mutation</span>
        <span class="c1"># and will preserve ops relying on the un-mutated tensors.</span>
        <span class="c1">#</span>
        <span class="c1"># These placeholder tensors are never publicly-available and thus cannot</span>
        <span class="c1"># be involved directly in future in-place updates</span>

        <span class="c1"># In Tensor._op, any tensor entering an op has its grad/view-info cleared</span>
        <span class="c1"># We must do this here up front since we need to consume information</span>
        <span class="c1"># about ``self``</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">null_grad</span><span class="p">(</span><span class="n">_clear_view_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">_view_children</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">graph</span> <span class="o">=</span> <span class="n">_dup</span><span class="o">.</span><span class="n">DuplicatingGraph</span><span class="p">(</span><span class="bp">self</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">)</span>

        <span class="c1"># Create copy of base so that mutation has no impact on the</span>
        <span class="c1"># state of any ops depending on it or its views</span>
        <span class="n">mutant_base</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">mutant_base</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">writeable</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">writeable</span>
            <span class="ow">or</span> <span class="n">_mem</span><span class="o">.</span><span class="n">array_is_tracked</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Create view of base in correspondence to relationship</span>
        <span class="c1"># that `self` has to base. Mutating this view will mutate</span>
        <span class="c1"># base appropriately</span>
        <span class="n">inplace_target</span> <span class="o">=</span> <span class="n">mutant_base</span>

        <span class="c1"># stores view-fn sequence from base -&gt; in-place target</span>
        <span class="n">view_fn_sequence</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">with</span> <span class="n">_track</span><span class="o">.</span><span class="n">no_autodiff</span><span class="p">:</span>
            <span class="c1"># get view sequence from base -&gt; in-place target</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_path_to_base</span><span class="p">(</span><span class="bp">self</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:]:</span>  <span class="c1"># skip base</span>
                <span class="c1"># need to point to place-holder replay op to avoid creating</span>
                <span class="c1"># forwards references to downstream tensors</span>
                <span class="n">f</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">_replay_op</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># need sequence of view-ops</span>
                    <span class="n">view_fn_sequence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_track</span><span class="o">.</span><span class="n">no_autodiff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="n">inplace_target</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">inplace_target</span><span class="p">)</span>

        <span class="c1"># Constant info was not propagated through no-autodiff mode.</span>
        <span class="c1"># It must be inferred from the original tensor</span>
        <span class="n">inplace_target</span><span class="o">.</span><span class="n">_constant</span> <span class="o">=</span> <span class="n">mutant_base</span><span class="o">.</span><span class="n">constant</span>

        <span class="n">mutant_base_data</span> <span class="o">=</span> <span class="n">mutant_base</span><span class="o">.</span><span class="n">data</span>
        <span class="k">del</span> <span class="n">mutant_base</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">_mem</span><span class="o">.</span><span class="n">mem_guard_off</span><span class="p">:</span>
                <span class="n">placeholder_mutant_view</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>  <span class="c1"># will raise if original data not writeable</span>
                        <span class="n">inplace_op</span><span class="p">,</span>
                        <span class="o">*</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_placeholder_if_exists</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_vars</span><span class="p">),</span>
                        <span class="n">op_args</span><span class="o">=</span><span class="n">op_args</span><span class="p">,</span>
                        <span class="n">op_kwargs</span><span class="o">=</span><span class="n">op_kwargs</span><span class="p">,</span>
                        <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
                        <span class="n">out</span><span class="o">=</span><span class="n">inplace_target</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">restore_old_graph</span><span class="p">()</span>
            <span class="k">raise</span> <span class="n">e</span>

        <span class="n">placeholder_mutant_view</span><span class="o">.</span><span class="n">_constant</span> <span class="o">=</span> <span class="n">inplace_target</span><span class="o">.</span><span class="n">_constant</span>

        <span class="k">if</span> <span class="n">_mem</span><span class="o">.</span><span class="n">MEM_GUARD</span><span class="p">:</span>
            <span class="n">_mem</span><span class="o">.</span><span class="n">force_lock_tensor_and_creators</span><span class="p">(</span><span class="n">placeholder_mutant_view</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">placeholder_mutant_view</span><span class="o">.</span><span class="n">creator</span><span class="o">.</span><span class="n">where</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># An operation like `multiply(x, y, where=mask, out=z)` occurred.</span>
            <span class="c1"># `placeholder_mutant_view` is the mutated version of `z`.</span>
            <span class="c1"># We need to connect the upstream version of `z` to the computational</span>
            <span class="c1"># graph so that `~mask * dâ„’/dz` backprops to it, whereas `~mask * dâ„’/dz`</span>
            <span class="c1"># will backprop to `x` and `y`.</span>
            <span class="c1">#</span>
            <span class="c1"># This is basically an alternative to treating</span>
            <span class="c1"># `multiply(x, y, where=mask, out=z)`</span>
            <span class="c1"># like a three-input operation, which adds complexity to the implementation</span>
            <span class="c1"># of every op that supports `where` and `out`.</span>
            <span class="c1">#</span>
            <span class="c1">#               old-z ---------------------</span>
            <span class="c1">#                 |                       |</span>
            <span class="c1">#   multiply(x, y, where=mask, out=z)     |</span>
            <span class="c1">#                 |                       |</span>
            <span class="c1">#                 z    --------------------</span>
            <span class="c1">#                 |    |</span>
            <span class="c1">#                 ApplyMask</span>
            <span class="c1">#                    |</span>
            <span class="c1">#                    z</span>
            <span class="k">with</span> <span class="n">_mem</span><span class="o">.</span><span class="n">mem_guard_off</span><span class="p">:</span>
                <span class="n">placeholder_mutant_view</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
                    <span class="n">_dup</span><span class="o">.</span><span class="n">ApplyMask</span><span class="p">,</span>
                    <span class="n">placeholder_mutant_view</span><span class="p">,</span>  <span class="c1"># gets passed through unchanged</span>
                    <span class="c1"># ~mask * grad  backprops to upstream placeholder</span>
                    <span class="n">graph</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span><span class="o">.</span><span class="n">placeholder</span><span class="p">,</span>
                    <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">&quot;mask&quot;</span><span class="p">:</span> <span class="n">placeholder_mutant_view</span><span class="o">.</span><span class="n">creator</span><span class="o">.</span><span class="n">where</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">)</span>

        <span class="c1"># Connect public base tensor to placeholder graph via the mutated placeholder</span>
        <span class="c1"># tensor `out`.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># The current graph:</span>
            <span class="c1">#    base-p --&gt; | inplace | --&gt; vp&#39;</span>
            <span class="c1"># Becomes:</span>
            <span class="c1">#    base-p --&gt; | inplace | --&gt; base&#39;</span>
            <span class="c1">#</span>
            <span class="c1"># The base tensor itself was the target of the in-place operation,</span>
            <span class="c1"># thus we need simply mirror original base against the mutant placeholder.</span>
            <span class="c1"># This effectively connects the original base to the placeholder graph</span>
            <span class="n">mutant_base</span> <span class="o">=</span> <span class="n">placeholder_mutant_view</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># in-place operation occurred on a view; must connect mutated base</span>
            <span class="c1"># to graph and then reproduce downstream views</span>
            <span class="c1">#</span>
            <span class="c1"># The current graph:</span>
            <span class="c1">#    vp --&gt; | inplace | --&gt; vp&#39;</span>
            <span class="c1">#</span>
            <span class="c1"># Becomes:</span>
            <span class="c1">#</span>
            <span class="c1">#    vp --&gt; | inplace | --&gt; vp&#39; --&gt; |        |</span>
            <span class="c1">#                                   | unview | --&gt; base&#39;</span>
            <span class="c1">#   base-p -----------------------&gt; |        |</span>
            <span class="c1">#</span>
            <span class="c1"># I.e. the mutated base is a combination of the placeholder</span>
            <span class="c1"># base and of the mutant view.</span>

            <span class="n">mutant_base</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
                <span class="n">_dup</span><span class="o">.</span><span class="n">UnView</span><span class="p">,</span>
                <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="p">,</span>
                <span class="n">placeholder_mutant_view</span><span class="p">,</span>
                <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span>
                    <span class="c1"># Copy to avoid upstream placeholder mutant view sharing memory</span>
                    <span class="c1"># with downstream mutant base</span>
                    <span class="s2">&quot;mutant_base_data&quot;</span><span class="p">:</span> <span class="n">mutant_base_data</span><span class="p">,</span>
                    <span class="s2">&quot;view_fn_sequence&quot;</span><span class="p">:</span> <span class="n">view_fn_sequence</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">)</span>

        <span class="k">del</span> <span class="n">placeholder_mutant_view</span>

        <span class="c1"># The original base now points to the augmented array data</span>
        <span class="c1"># and has the InPlaceOp as its creator</span>
        <span class="n">_dup</span><span class="o">.</span><span class="n">mirror_tensor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">mutant_base</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>

        <span class="k">del</span> <span class="n">mutant_base</span>

        <span class="c1"># Now that the base-tensor has been incorporated into the graph,</span>
        <span class="c1"># recreate the view-graph and reroute all tensors from previous</span>
        <span class="c1"># graph to their downstream counterparts</span>
        <span class="c1">#</span>
        <span class="c1"># Note that iterating in a topologically-ordered way is critical</span>
        <span class="c1"># here: each parent is updated before creating one of its children</span>
        <span class="c1">#</span>
        <span class="c1"># Iteration is always based off of the placeholders&#39; relative positions</span>
        <span class="c1"># in the graph since this will never be mutated.</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">view</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">_replay_op</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">parent</span><span class="p">)</span>
            <span class="n">_dup</span><span class="o">.</span><span class="n">mirror_tensor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">view</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">node</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">_view_children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Shape</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tuple of tensor dimension-sizes.</span>

<span class="sd">        Sizes are reported in row-major order.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tuple[int, ...]</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([1, 2, 3, 4])  # axis-0 has size 4</span>
<span class="sd">        &gt;&gt;&gt; x.shape</span>
<span class="sd">        (4,)</span>
<span class="sd">        &gt;&gt;&gt; y = mg.Tensor([[1, 2, 3],    # axis-0 has size 2, axis-1 has size 3</span>
<span class="sd">        ...                [4, 5, 6]])</span>
<span class="sd">        &gt;&gt;&gt; y.shape</span>
<span class="sd">        (2, 3)</span>

<span class="sd">        The shape attribute can also be set to reshape the tensor in-place</span>

<span class="sd">        &gt;&gt;&gt; y.shape = (1, 6, 1)</span>
<span class="sd">        &gt;&gt;&gt; y</span>
<span class="sd">        Tensor([[[1],</span>
<span class="sd">                 [2],</span>
<span class="sd">                 [3],</span>
<span class="sd">                 [4],</span>
<span class="sd">                 [5],</span>
<span class="sd">                 [6]]])</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        mygrad.reshape : similar function</span>
<span class="sd">        Tensor.reshape : similar method&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>

    <span class="nd">@shape</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">newshape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Shape</span><span class="p">]):</span>
        <span class="c1"># Even though this op cannot mutate views, we still must</span>
        <span class="c1"># do graph-replaying here so that views can still reference</span>
        <span class="c1"># this tensor, but with the proper reshaping mediating them.</span>
        <span class="c1">#</span>
        <span class="c1"># E.g.</span>
        <span class="c1"># x = arange(10)   # shape-(10,)</span>
        <span class="c1"># y = x[:6]        # shape-(6,)</span>
        <span class="c1"># x.shape = (2, 5) # shape-(2, 5)</span>
        <span class="c1">#</span>
        <span class="c1"># y.base points to the shape-(2,5) array</span>
        <span class="c1"># even though y is a view of the flat array</span>
        <span class="c1">#</span>
        <span class="c1"># thus we need to play this graph as</span>
        <span class="c1">#   (history)</span>
        <span class="c1">#       |</span>
        <span class="c1">#   placeholder   shape-(10,)</span>
        <span class="c1">#       |-reshape</span>
        <span class="c1">#       x         shape-(2,5)</span>
        <span class="c1">#       |-reshape</span>
        <span class="c1">#   placeholder   shape-(10,)</span>
        <span class="c1">#       |-getitem</span>
        <span class="c1">#       y         shape-(4,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_track</span><span class="o">.</span><span class="n">TRACK_GRAPH</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">newshape</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">newshape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">old_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># raise here if the shape is not compatible</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">newshape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">old_shape</span>

        <span class="c1"># create placeholders for self and all of its view-children</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">_dup</span><span class="o">.</span><span class="n">DuplicatingGraph</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># need to iterate over all nodes now before we tinker</span>
        <span class="c1"># with the view children</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>

        <span class="c1"># reshape placeholder of self</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">newshape</span><span class="p">)</span>

        <span class="c1"># Store contents of `out` in `self` and replace `out` in</span>
        <span class="c1"># graph with `self`</span>
        <span class="n">out</span><span class="o">.</span><span class="n">_base</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">base</span>
        <span class="n">_dup</span><span class="o">.</span><span class="n">mirror_tensor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">_dup</span><span class="o">.</span><span class="n">reroute_ops_through</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">out</span>

        <span class="c1"># although `self` is a view of placeholder, placeholder</span>
        <span class="c1"># is strictly an internal tensor, we won&#39;t expose it as</span>
        <span class="c1"># base</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">_view_children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">base</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">base</span>

        <span class="k">if</span> <span class="n">base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if `self` was a view, we need to update that parent&#39;s</span>
            <span class="c1"># view children so that it points to the placeholder</span>
            <span class="n">creator</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">creator</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">creator</span><span class="o">.</span><span class="n">_view_children</span> <span class="o">=</span> <span class="n">WeakRefIterable</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">w</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="k">else</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span>
                    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">_view_children</span>
                <span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># Undo the reshape, and place this as the tensor joining</span>
        <span class="c1"># the reshaped `self` with the views of unshaped `self`</span>
        <span class="n">unshaped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">old_shape</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="c1"># direct what would be views of `self` to be views of `unshaped`,</span>
            <span class="c1"># which translates the mutated shape of `self` to the original</span>
            <span class="c1"># shape used to create the views</span>
            <span class="n">parent</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parent</span> <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="k">else</span> <span class="n">unshaped</span>
            <span class="n">view</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">_replay_op</span><span class="p">(</span><span class="n">parent</span><span class="p">)</span>
            <span class="n">_dup</span><span class="o">.</span><span class="n">mirror_tensor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">view</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">_dup</span><span class="o">.</span><span class="n">reroute_ops_through</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">view</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">parent</span><span class="o">.</span><span class="n">_view_children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Index</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">SetItem</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">key</span><span class="p">,))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Add</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Subtract</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Subtract</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Subtract</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Divide</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Divide</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Divide</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Multiply</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Multiply</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Multiply</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">MatMul</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">MatMul</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">other</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Positive</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">other</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Square</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Power</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">other</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Positive</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span>
            <span class="k">elif</span> <span class="n">other</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Square</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_in_place_op</span><span class="p">(</span><span class="n">Power</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Power</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Negative</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__pos__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Positive</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;array&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> &quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Produces a copy of ``self`` with ``copy.creator=None``.</span>

<span class="sd">        Copies of the underlying numpy data array and gradient array are created.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<div class="viewcode-block" id="Tensor.copy"><a class="viewcode-back" href="../../generated/mygrad.Tensor.copy.html#mygrad.Tensor.copy">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Produces a copy of ``self`` with ``copy.creator=None``.</span>

<span class="sd">        Copies of the underlying numpy data array and gradient array are created.</span>

<span class="sd">        No information regarding the tensor&#39;s participation in the computational</span>
<span class="sd">        graph are copied.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        constant : Optional[bool]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor(data, constant=constant)</span>
<span class="sd">        &gt;&gt;&gt; y = x * 2</span>
<span class="sd">        &gt;&gt;&gt; y.backward()</span>
<span class="sd">        &gt;&gt;&gt; y_copy = y.copy()</span>
<span class="sd">        &gt;&gt;&gt; y_copy</span>
<span class="sd">        Tensor(6)</span>
<span class="sd">        &gt;&gt;&gt; y_copy.grad</span>
<span class="sd">        array(1.)</span>
<span class="sd">        &gt;&gt;&gt; y_copy.creator is None</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">copy</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
            <span class="n">constant</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="k">if</span> <span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">constant</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">copy</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">copy</span></div>

<div class="viewcode-block" id="Tensor.item"><a class="viewcode-back" href="../../generated/mygrad.Tensor.item.html#mygrad.Tensor.item">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">item</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Copy an element of a tensor to a standard Python scalar and return it.</span>

<span class="sd">        Note that the returned object does not support back-propagation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z : Standard Python scalar object</span>
<span class="sd">            A copy of the specified element of the tensor as a suitable</span>
<span class="sd">            Python scalar</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([22.2])</span>
<span class="sd">        &gt;&gt;&gt; x.item()</span>
<span class="sd">        22.2</span>
<span class="sd">        &gt;&gt;&gt; type(x.item())</span>
<span class="sd">        float&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;can only convert a tensor of size 1 to a Python scalar&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>

    <span class="k">def</span><span class="w"> </span><span class="fm">__float__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;can only convert a tensor of size 1 to a Python scalar&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;can only convert a tensor of size 1 to a Python scalar&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__index__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return self converted to an integer, if self is suitable for use as an index</span>
<span class="sd">        into a list.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="fm">__index__</span><span class="p">()</span>

<div class="viewcode-block" id="Tensor.flatten"><a class="viewcode-back" href="../../generated/mygrad.Tensor.flatten.html#mygrad.Tensor.flatten">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">flatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a copy of the tensor collapsed into one dimension.</span>

<span class="sd">        This docstring was adapted from ``numpy.ndarray.flatten``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        constant : bool, optional(default=False)</span>
<span class="sd">            If ``True``, the returned tensor is a constant (it</span>
<span class="sd">            does not back-propagate a gradient)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>
<span class="sd">            A copy of the input tensor, flattened to one dimension.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        To return a flattened view of the tensor, use ``x.reshape(-1)``.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([[1, 2],</span>
<span class="sd">        ...                [3, 4]])</span>
<span class="sd">        &gt;&gt;&gt; x.flatten()</span>
<span class="sd">        Tensor([1, 2, 3, 4])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Flatten</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">base</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Tensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A reference to the base tensor that the present tensor is a view of.</span>

<span class="sd">        It this tensor owns its memory, then this returns ``None``.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The base of a tensor that owns its memory is ``None``:</span>

<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.arange(5)</span>
<span class="sd">        &gt;&gt;&gt; x.base is None</span>
<span class="sd">        True</span>

<span class="sd">        Slicing creates a view, whose memory is shared with x:</span>

<span class="sd">        &gt;&gt;&gt; y = x[2:]</span>
<span class="sd">        &gt;&gt;&gt; y.base is x</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; y.data.base is x.data</span>
<span class="sd">        True</span>

<span class="sd">        A view of a view has the same base as its &quot;parent&quot;</span>

<span class="sd">        &gt;&gt;&gt; z = y[:]</span>
<span class="sd">        &gt;&gt;&gt; z.base is x</span>
<span class="sd">        True</span>

<span class="sd">        The behavior of ``Tensor.base`` departs from that of ``ndarray.base`` in that</span>
<span class="sd">        mygrad will never create an &quot;internal&quot; tensor to serve as a base; e.g.</span>

<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; np.reshape(2., (1,)).base</span>
<span class="sd">        array(2.)</span>

<span class="sd">        &gt;&gt;&gt; mg.reshape(2., (1,)).base is None</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Number of elements in the tensor. i.e., the product of the tensor&#39;s</span>
<span class="sd">        dimensions.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.zeros((3, 5, 2))  # creates a tensor with 3x5x2 (= 30) elements</span>
<span class="sd">        &gt;&gt;&gt; x.size</span>
<span class="sd">        30</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of tensor dimensions. I.e. the number</span>
<span class="sd">        of indices that must be supplied to uniquely specify</span>
<span class="sd">        an element in the tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; x.ndim</span>
<span class="sd">        1</span>
<span class="sd">        &gt;&gt;&gt; x[0]  # a single index identifies an element in `x`</span>
<span class="sd">        Tensor(1)</span>

<span class="sd">        &gt;&gt;&gt; y = mg.Tensor([[1, 2, 3],</span>
<span class="sd">        ...                [4, 5, 6]])</span>
<span class="sd">        &gt;&gt;&gt; y.ndim</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; y[0, 0]  # two indices are required to identify an element in `x`</span>
<span class="sd">        Tensor(1)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ndim</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data-type of the tensor&#39;s elements.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy dtype object</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; x = mg.Tensor([[0, 1],</span>
<span class="sd">        ...                [2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; x.dtype</span>
<span class="sd">        dtype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; type(x.dtype)</span>
<span class="sd">        &lt;type &#39;numpy.dtype&#39;&gt;&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reshape</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">newshape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Shape</span><span class="p">],</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a tensor with a new shape, without changing its data.</span>
<span class="sd">        This docstring was adapted from ``numpy.reshape``</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *newshape : Union[int, Tuple[int, ...]]</span>
<span class="sd">            The new shape should be compatible with the original shape. If</span>
<span class="sd">            an integer, then the result will be a 1-D tensor of that length.</span>
<span class="sd">            One shape dimension can be -1. In this case, the value is</span>
<span class="sd">            inferred from the length of the tensor and remaining dimensions.</span>

<span class="sd">        constant : bool, optional(default=False)</span>
<span class="sd">            If ``True``, the returned tensor is a constant (it</span>
<span class="sd">            does not back-propagate a gradient)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>
<span class="sd">            ``a`` with its shape changed.  A new tensor is returned.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        ``reshape`` utilizes C-ordering, meaning that it reads &amp; writes elements using</span>
<span class="sd">        C-like index ordering; the last axis index changing fastest, and, proceeding</span>
<span class="sd">        in reverse order, the first axis index changing slowest.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; a = mg.Tensor([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">        &gt;&gt;&gt; a.reshape(6)</span>
<span class="sd">        Tensor([1, 2, 3, 4, 5, 6])</span>

<span class="sd">        &gt;&gt;&gt; a.reshape(3, -1))   # the unspecified value is inferred to be 2</span>
<span class="sd">        Tensor([[1, 2],</span>
<span class="sd">                [3, 4],</span>
<span class="sd">                [5, 6]])</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">newshape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;reshape() takes at least 1 argument (0 given)&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">newshape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;__iter__&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">newshape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;an integer is required&quot;</span><span class="p">)</span>
            <span class="n">newshape</span> <span class="o">=</span> <span class="n">newshape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Reshape</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">newshape</span><span class="p">,),</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Same as self.transpose(), except that self is returned if self.ndim &lt; 2 and</span>
<span class="sd">        a view of the underlying data is utilized whenever possible.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; y = mg.Tensor([[1, 2, 3],</span>
<span class="sd">        ...                [4, 5, 6]])</span>
<span class="sd">        &gt;&gt;&gt; y.T</span>
<span class="sd">        Tensor([[1, 4],</span>
<span class="sd">                [2, 5],</span>
<span class="sd">                [3, 6]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Tensor_Transpose_Property</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asarray</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asarray</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asarray</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asarray</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asarray</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asarray</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__imatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># pragma: no cover</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;In-place matrix multiplication is not (yet) supported. &quot;</span>
            <span class="s2">&quot;Use &#39;a = a @ b&#39; instead of &#39;a @= b&#39;&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sum of tensor elements over a given axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[int, Tuple[ints, ...]]</span>
<span class="sd">            Axis or axes along which a sum is performed.  The default,</span>
<span class="sd">            axis=None, will sum all of the elements of the input tensor.  If</span>
<span class="sd">            axis is negative it counts from the last to the first axis.</span>
<span class="sd">            If axis is a tuple of ints, a sum is performed on all of the axes</span>
<span class="sd">            specified in the tuple instead of a single axis or all the axes as</span>
<span class="sd">            before.</span>

<span class="sd">        keepdims : bool, optional</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the input tensor.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sum_along_axis : mygrad.Tensor</span>
<span class="sd">            A Tensor with the same shape as `self`, with the specified</span>
<span class="sd">            axis/axes removed. If `self` is a 0-d tensor, or if `axis` is None,</span>
<span class="sd">            a 0-dim Tensor is returned.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        mygrad.Tensor.sum : Equivalent method.</span>

<span class="sd">        cumsum : Cumulative sum of array elements.</span>

<span class="sd">        mean, average</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Arithmetic is modular when using integer types, and no error is</span>
<span class="sd">        raised on overflow.</span>

<span class="sd">        The sum of an empty tensor is the neutral element 0:</span>

<span class="sd">        &gt;&gt;&gt; mygrad.sum([])</span>
<span class="sd">        Tensor(0.0)</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = mg.tensor([1., 1.])</span>
<span class="sd">        &gt;&gt;&gt; x.sum()</span>
<span class="sd">        Tensor(2.0)</span>
<span class="sd">        &gt;&gt;&gt; x = mg.tensor([0.5, 0.7, 0.2, 1.5])</span>
<span class="sd">        &gt;&gt;&gt; x.sum(dtype=np.int32)</span>
<span class="sd">        Tensor(1)</span>
<span class="sd">        &gt;&gt;&gt; x = mg.tensor([[0, 1], [0, 5]])</span>
<span class="sd">        &gt;&gt;&gt; x.sum()</span>
<span class="sd">        Tensor(6)</span>
<span class="sd">        &gt;&gt;&gt; x.sum(axis=0)</span>
<span class="sd">        Tensor([0, 6])</span>
<span class="sd">        &gt;&gt;&gt; x.sum(axis=1)</span>
<span class="sd">        Tensor([1, 5])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">Sum</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">},</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the product of array elements over given axes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[Union[int, Tuple[int, ...]]]</span>
<span class="sd">            Axis or axes along which to operate. By default, flattened input is used.</span>

<span class="sd">        keepdims : bool, optional (default=False)</span>
<span class="sd">            If this is set to True, the axes which are reduced are left in the</span>
<span class="sd">            result as dimensions with size one. With this option, the result</span>
<span class="sd">            will broadcast correctly against the input array.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        product_along_axis : mygrad.Tensor</span>
<span class="sd">            A tensor shaped as `a` but with the specified axis removed.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">Prod</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">},</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cumprod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the cumulative product of elements along a given axis.</span>

<span class="sd">        This docstring was adapted from the official numpy documentation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[int]</span>
<span class="sd">            Axis along which the cumulative product is computed.  By default</span>
<span class="sd">            the input is flattened.</span>

<span class="sd">        constant : bool, optional(default=False)</span>
<span class="sd">            If ``True``, the returned tensor is a constant (it</span>
<span class="sd">            does not back-propagate a gradient)</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Arithmetic is modular when using integer types, and no error is</span>
<span class="sd">        raised on overflow.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">CumProd</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">},</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the cumulative sum of the elements along a given axis.</span>

<span class="sd">        This docstring was adapted from the official numpy documentation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, optional</span>
<span class="sd">            Axis along which the cumulative sum is computed. The default</span>
<span class="sd">            (None) is to compute the cumsum over the flattened array.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">CumSum</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">},</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Mean of tensor elements over a given axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : ArrayLike</span>

<span class="sd">        axis : Optional[int, Tuple[ints, ...]</span>
<span class="sd">            Axis or axes along which a mean is performed.  The default,</span>
<span class="sd">            axis=None, will mean all of the elements of the input tensor.  If</span>
<span class="sd">            axis is negative it counts from the last to the first axis.</span>

<span class="sd">            If axis is a tuple of ints, a mean is performed on all of the axes</span>
<span class="sd">            specified in the tuple instead of a single axis or all the axes as</span>
<span class="sd">            before.</span>

<span class="sd">        keepdims : bool, optional</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the input tensor.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mean_along_axis : Tensor</span>
<span class="sd">            A Tensor with the same shape as `self`, with the specified</span>
<span class="sd">            axis/axes removed. If `self` is a 0-d tensor, or if `axis` is None,</span>
<span class="sd">            a 0-dim Tensor is returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">Mean</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">},</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ddof</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the standard deviation along the specified axis.</span>

<span class="sd">        Returns the variance of the array elements, a measure of the spread of a</span>
<span class="sd">        distribution.  The variance is computed for the flattened array by</span>
<span class="sd">        default, otherwise over the specified axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[Union[int, Tuple[int, ...]]]</span>
<span class="sd">            Axis or axes along which the variance is computed.  The default is to</span>
<span class="sd">            compute the variance of the flattened array.</span>

<span class="sd">        ddof : int, optional (default=0)</span>
<span class="sd">            &quot;Delta Degrees of Freedom&quot;: the divisor used in the calculation is</span>
<span class="sd">            ``N - ddof``, where ``N`` represents the number of elements. By</span>
<span class="sd">            default `ddof` is zero.</span>

<span class="sd">        keepdims : bool, optional (default=False)</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the input array.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        std : mygrad.Tensor</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The variance is the average of the squared deviations from the mean,</span>
<span class="sd">        i.e.,  ``var = mean(abs(x - x.mean())**2)``.</span>

<span class="sd">        The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.</span>
<span class="sd">        If, however, `ddof` is specified, the divisor ``N - ddof`` is used</span>
<span class="sd">        instead.  In standard statistical practice, ``ddof=1`` provides an</span>
<span class="sd">        unbiased estimator of the variance of a hypothetical infinite population.</span>
<span class="sd">        ``ddof=0`` provides a maximum likelihood estimate of the variance for</span>
<span class="sd">        normally distributed variables.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">StdDev</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span> <span class="s2">&quot;ddof&quot;</span><span class="p">:</span> <span class="n">ddof</span><span class="p">},</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ddof</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the variance along the specified axis.</span>

<span class="sd">        Returns the variance of the array elements, a measure of the spread of a</span>
<span class="sd">        distribution.  The variance is computed for the flattened array by</span>
<span class="sd">        default, otherwise over the specified axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[int, Tuple[int, ...]]</span>
<span class="sd">            Axis or axes along which the variance is computed.  The default is to</span>
<span class="sd">            compute the variance of the flattened array.</span>

<span class="sd">        ddof : int, optional (default=0)</span>
<span class="sd">            &quot;Delta Degrees of Freedom&quot;: the divisor used in the calculation is</span>
<span class="sd">            ``N - ddof``, where ``N`` represents the number of elements. By</span>
<span class="sd">            default `ddof` is zero.</span>

<span class="sd">        keepdims : bool, optional (default=False)</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the input array..</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        variance : mygrad.Tensor</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The variance is the average of the squared deviations from the mean,</span>
<span class="sd">        i.e.,  ``var = mean(abs(x - x.mean())**2)``.</span>

<span class="sd">        The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.</span>
<span class="sd">        If, however, `ddof` is specified, the divisor ``N - ddof`` is used</span>
<span class="sd">        instead.  In standard statistical practice, ``ddof=1`` provides an</span>
<span class="sd">        unbiased estimator of the variance of a hypothetical infinite population.</span>
<span class="sd">        ``ddof=0`` provides a maximum likelihood estimate of the variance for</span>
<span class="sd">        normally distributed variables.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">Variance</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span> <span class="s2">&quot;ddof&quot;</span><span class="p">:</span> <span class="n">ddof</span><span class="p">},</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the maximum of a tensor or maximum along its axes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : ArrayLike</span>

<span class="sd">        axis : Optional[int, Tuple[int, ...]]</span>
<span class="sd">            Axis or axes along which to operate. By default, flattened input is used.</span>

<span class="sd">        keepdims : bool, optional</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `arr`.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        max : mygrad.Tensor</span>
<span class="sd">            Maximum of `a`. If `axis` is None, the result is a 0-D tensor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = mg.arange(4).reshape((2,2))</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        Tensor([[0, 1],</span>
<span class="sd">                [2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; mg.amax(a)           # Maximum of the flattened array</span>
<span class="sd">        Tensor(3)</span>
<span class="sd">        &gt;&gt;&gt; mg.amax(a, axis=0)   # Maxima along the first axis</span>
<span class="sd">        Tensor([2, 3])</span>
<span class="sd">        &gt;&gt;&gt; mg.amax(a, axis=1)   # Maxima along the second axis</span>
<span class="sd">        Tensor([1, 3])</span>
<span class="sd">        &gt;&gt;&gt; b = mg.arange(5, dtype=float)</span>
<span class="sd">        &gt;&gt;&gt; b[2] = np.NaN</span>
<span class="sd">        &gt;&gt;&gt; mg.amax(b)</span>
<span class="sd">        Tensor(nan)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">Max</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">_NoValue</span><span class="p">},</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the minimum of a tensor or minimum along its axes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[int, Tuple[int, ...]]</span>
<span class="sd">            Axis or axes along which to operate. By default, flattened input is used.</span>

<span class="sd">        keepdims : bool, optional</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `arr`.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        min : mygrad.Tensor</span>
<span class="sd">            Minimum of `a`. If `axis` is None, the result is a 0-D tensor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = mg.arange(4).reshape((2,2))</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        Tensor([[0, 1],</span>
<span class="sd">                [2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; mg.amin(a)           # Minimum of the flattened array</span>
<span class="sd">        Tensor(0)</span>
<span class="sd">        &gt;&gt;&gt; mg.amin(a, axis=0)   # Minima along the first axis</span>
<span class="sd">        Tensor([0, 1])</span>
<span class="sd">        &gt;&gt;&gt; mg.amin(a, axis=1)   # Minima along the second axis</span>
<span class="sd">        Tensor([0, 2])</span>
<span class="sd">        &gt;&gt;&gt; b = mg.arange(5, dtype=float)</span>
<span class="sd">        &gt;&gt;&gt; b[2] = np.NaN</span>
<span class="sd">        &gt;&gt;&gt; mg.amin(b)</span>
<span class="sd">        Tensor(nan)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">Min</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">_NoValue</span><span class="p">},</span>
            <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">swapaxes</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">axis2</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Interchange two axes of a tensor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis1 : int</span>
<span class="sd">            First axis.</span>

<span class="sd">        axis2 : int</span>
<span class="sd">            Second axis.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">SwapAxes</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">),</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">transpose</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="o">*</span><span class="n">axes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Permute the dimensions of a tensor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axes : int</span>
<span class="sd">            By default, reverse the dimensions, otherwise permute the axes</span>
<span class="sd">            according to the values given.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>
<span class="sd">            `a` with its axes permuted.  A new tensor is returned.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; a = mg.tensor([[1, 2], [3, 4]])</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        Tensor([[1, 2],</span>
<span class="sd">                [3, 4]])</span>
<span class="sd">        &gt;&gt;&gt; a.transpose()</span>
<span class="sd">        Tensor([[1, 3],</span>
<span class="sd">                [2, 4]])</span>
<span class="sd">        &gt;&gt;&gt; a.transpose((1, 0))</span>
<span class="sd">        Tensor([[1, 3],</span>
<span class="sd">                [2, 4]])</span>
<span class="sd">        &gt;&gt;&gt; a.transpose(1, 0)</span>
<span class="sd">        Tensor([[1, 3],</span>
<span class="sd">                [2, 4]])&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">axes</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;__iter__&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&#39; object cannot be interpreted as an integer&quot;</span>
                <span class="p">)</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Transpose</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">axes</span><span class="p">,),</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">moveaxis</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="n">destination</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move axes of a tensor to new positions. Other axes remain in their</span>
<span class="sd">        original order.</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        source : Union[int, Sequence[int]]</span>
<span class="sd">            Original positions of the axes to move. These must be unique.</span>

<span class="sd">        destination : Union[int, Sequence[int]]</span>
<span class="sd">            Destination positions for each of the original axes. These must also be</span>
<span class="sd">            unique.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        result : mygrad.Tensor</span>
<span class="sd">            Array with moved axes. This array is a view of the input array..&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span>
            <span class="n">MoveAxis</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">),</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">squeeze</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove single-dimensional entries from the shape of a tensor.</span>

<span class="sd">        This docstring was adapted from ``numpy.squeeze``</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : Optional[int, Tuple[int, ...]]</span>
<span class="sd">            Selects a subset of the single-dimensional entries in the</span>
<span class="sd">            shape. If an axis is selected with shape entry greater than</span>
<span class="sd">            one, an error is raised.</span>

<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>


<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If ``axis`` is not ``None``, and an axis being squeezed is not of length 1</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Squeeze</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">op_args</span><span class="o">=</span><span class="p">(</span><span class="n">axis</span><span class="p">,),</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">ravel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Flattens contents of a tensor into a contiguous 1-D array.  A copy is made only if needed.</span>

<span class="sd">        This docstring was adapted from ``numpy.ravel``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        constant : Optional[bool]</span>
<span class="sd">            If ``True``, this tensor is treated as a constant, and thus does not</span>
<span class="sd">            facilitate back propagation (i.e. ``constant.grad`` will always return</span>
<span class="sd">            ``None``).</span>

<span class="sd">            Defaults to ``False`` for float-type data.</span>
<span class="sd">            Defaults to ``True`` for integer-type data.</span>

<span class="sd">            Integer-type tensors must be constant.</span>


<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mygrad.Tensor</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        ``ravel`` utilizes C-ordering, meaning that it reads &amp; writes elements using</span>
<span class="sd">        C-like index ordering; the last axis index changing fastest, and, proceeding</span>
<span class="sd">        in reverse order, the first axis index changing slowest.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="n">Ravel</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">argmax</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the indices of the maximum values along an axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a: array_like</span>

<span class="sd">        axis: int, optional</span>
<span class="sd">            By default, the index is into the flattened array, otherwise along the specified axis.</span>

<span class="sd">        out: numpy.array, optional</span>
<span class="sd">            If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.ndarray[int]&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">argmin</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the indices of the minimum values along an axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: int, optional</span>
<span class="sd">            By default, the index is into the flattened array, otherwise along the specified axis.</span>

<span class="sd">        out: numpy.array, optional</span>
<span class="sd">            If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.ndarray[int]&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">any</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Test whether any array or Tensor element along a given axis evaluates to True.</span>

<span class="sd">        Returns single boolean if `axis` is ``None``</span>

<span class="sd">        This documentation was adapted from ``numpy.add``</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : None or int or tuple of ints, optional</span>
<span class="sd">            Axis or axes along which a logical OR reduction is performed.</span>
<span class="sd">            The default (``axis=None``) is to perform a logical OR over all</span>
<span class="sd">            the dimensions of the input array. `axis` may be negative, in</span>
<span class="sd">            which case it counts from the last to the first axis.</span>
<span class="sd">            If this is a tuple of ints, a reduction is performed on multiple</span>
<span class="sd">            axes, instead of a single axis or all the axes as before.</span>

<span class="sd">        out : ndarray, optional</span>
<span class="sd">            Alternate output array in which to place the result.  It must have</span>
<span class="sd">            the same shape as the expected output and its type is preserved</span>
<span class="sd">            (e.g., if it is of type float, then it will remain so, returning</span>
<span class="sd">            1.0 for True and 0.0 for False, regardless of the type of `a`).</span>
<span class="sd">            See `ufuncs-output-type` for more details.</span>

<span class="sd">        keepdims : bool, optional</span>
<span class="sd">            If this is set to True, the axes which are reduced are left</span>
<span class="sd">            in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the input array.</span>
<span class="sd">            If the default value is passed, then `keepdims` will not be</span>
<span class="sd">            passed through to the `any` method of sub-classes of</span>
<span class="sd">            `ndarray`, however any non-default value will be.  If the</span>
<span class="sd">            sub-class&#39; method does not implement `keepdims` any</span>
<span class="sd">            exceptions will be raised.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        any : bool or ndarray</span>
<span class="sd">            A new boolean or `ndarray` is returned unless `out` is specified,</span>
<span class="sd">            in which case a reference to `out` is returned.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        Tensor.any : equivalent method</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">clip</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">a_min</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">a_max</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Clip (limit) the values in an array.</span>

<span class="sd">        Given an interval, values outside the interval are clipped to</span>
<span class="sd">        the interval edges.  For example, if an interval of ``[0, 1]``</span>
<span class="sd">        is specified, values smaller than 0 become 0, and values larger</span>
<span class="sd">        than 1 become 1.</span>

<span class="sd">        Equivalent to `mg.minimum(a_max, mg.maximum(a, a_min))``.</span>

<span class="sd">        No check is performed to ensure ``a_min &lt; a_max``.</span>

<span class="sd">        This docstring was adapted from that of `numpy.clip`</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a_min : Optional[float, ArrayLike]</span>
<span class="sd">            Minimum value. If `None`, clipping is not performed on lower</span>
<span class="sd">            interval edge. Not more than one of `a_min` and `a_max` may be</span>
<span class="sd">            `None`.</span>

<span class="sd">        a_max : Optional[float, ArrayLike]</span>
<span class="sd">            Maximum value. If `None`, clipping is not performed on upper</span>
<span class="sd">            interval edge. Not more than one of `a_min` and `a_max` may be</span>
<span class="sd">            `None`. If `a_min` or `a_max` are ArrayLike, then the three</span>
<span class="sd">            arrays will be broadcasted to match their shapes.</span>

<span class="sd">        out : Optional[Union[ndarray, Tensor]]</span>
<span class="sd">            A location into which the result is stored. If provided, it must have</span>
<span class="sd">            a shape that the inputs broadcast to. If not provided or None, a</span>
<span class="sd">            freshly-allocated tensor is returned.</span>

<span class="sd">        constant : bool, optional(default=False)</span>
<span class="sd">            If ``True``, the returned tensor is a constant (it</span>
<span class="sd">            does not backpropagate a gradient)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            A tensor with the elements of `a`, but where values</span>
<span class="sd">            &lt; `a_min` are replaced with `a_min`, and those &gt; `a_max`</span>
<span class="sd">            with `a_max`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import mygrad as mg</span>
<span class="sd">        &gt;&gt;&gt; a = mg.arange(10)</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
<span class="sd">        &gt;&gt;&gt; a.clip(1, 8)</span>
<span class="sd">        Tensor([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])</span>
<span class="sd">        &gt;&gt;&gt; a.clip([3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)</span>
<span class="sd">        Tensor([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])&quot;&quot;&quot;</span>
        <span class="c1"># set in added in mygrad.__init__</span>
        <span class="o">...</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      Â© Copyright 2023, Ryan Soklaski.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.0.1.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>